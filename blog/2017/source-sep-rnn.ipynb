{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sequence(start=0, length =80, p=0.95):\n",
    "    sequence = np.zeros(length)\n",
    "    sequence[0] = start\n",
    "    for i in range(1, length):\n",
    "        random_num = np.random.rand()\n",
    "        if random_num > p:\n",
    "            # Switch state\n",
    "            sequence[i] = 1-sequence[i-1]\n",
    "        else:\n",
    "            sequence[i] = sequence[i-1]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "source_1_range = [100, 200]\n",
    "source_2_range = [1000, 1300]\n",
    "\n",
    "sequence_length = 48\n",
    "num_sequences = 1000\n",
    "\n",
    "seq1 = []\n",
    "seq2 = []\n",
    "combined = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for seq in range(num_sequences):\n",
    "    if seq % (num_sequences/10)==0:\n",
    "        print(seq)\n",
    "    source_1_power_val = np.random.choice(list(range(*source_1_range)))\n",
    "    source_2_power_val = np.random.choice(list(range(*source_2_range)))\n",
    "    \n",
    "    source_1_seq = source_1_power_val*gen_sequence(start=np.random.choice([0, 1]), length =sequence_length, p=0.95)\n",
    "    source_2_seq = source_2_power_val*gen_sequence(start=np.random.choice([0, 1]), length =sequence_length, p=0.95)\n",
    "    combined_seq = source_1_seq + source_2_seq\n",
    "    seq1.append(source_1_seq)\n",
    "    seq2.append(source_2_seq)\n",
    "    combined.append(combined_seq)\n",
    "\n",
    "combined = np.array(combined)\n",
    "seq1 = np.array(seq1)\n",
    "seq2 = np.array(seq2)\n",
    "\n",
    "#seq1 = seq1.reshape(num_sequences, sequence_length, 1)\n",
    "#seq2 = seq2.reshape(num_sequences, sequence_length, 1)\n",
    "#combined = combined.reshape(num_sequences, sequence_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 48), (1000, 48))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape, seq1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq1 = seq1.reshape(num_sequences, sequence_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, InputLayer, Activation, Dropout, MaxPooling1D\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 4,704\n",
      "Trainable params: 4,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=(sequence_length,)))\n",
    "\n",
    "\n",
    "model.add(Dense(sequence_length, activation='relu'))\n",
    "model.add(Dense(sequence_length, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile('adam','mean_absolute_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 268.68 296.00\" width=\"269pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-292 264.68,-292 264.68,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 103720883760 -->\n",
       "<g class=\"node\" id=\"node1\"><title>103720883760</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 260.68,-287.5 260.68,-243.5 0,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-261.3\">input_3: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"128.362,-243.5 128.362,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.197\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"128.362,-265.5 184.031,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.197\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"184.031,-243.5 184.031,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.355\" y=\"-272.3\">(None, 48)</text>\n",
       "<polyline fill=\"none\" points=\"184.031,-265.5 260.68,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.355\" y=\"-250.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 103720884320 -->\n",
       "<g class=\"node\" id=\"node2\"><title>103720884320</title>\n",
       "<polygon fill=\"none\" points=\"12.0552,-162.5 12.0552,-206.5 248.625,-206.5 248.625,-162.5 12.0552,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-180.3\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"116.307,-162.5 116.307,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.142\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"116.307,-184.5 171.976,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.142\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"171.976,-162.5 171.976,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210.3\" y=\"-191.3\">(None, 48)</text>\n",
       "<polyline fill=\"none\" points=\"171.976,-184.5 248.625,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210.3\" y=\"-169.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 103720883760&#45;&gt;103720884320 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>103720883760-&gt;103720884320</title>\n",
       "<path d=\"M130.34,-243.329C130.34,-235.183 130.34,-225.699 130.34,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"133.84,-216.729 130.34,-206.729 126.84,-216.729 133.84,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 103720884544 -->\n",
       "<g class=\"node\" id=\"node3\"><title>103720884544</title>\n",
       "<polygon fill=\"none\" points=\"12.0552,-81.5 12.0552,-125.5 248.625,-125.5 248.625,-81.5 12.0552,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-99.3\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"116.307,-81.5 116.307,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.142\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"116.307,-103.5 171.976,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.142\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"171.976,-81.5 171.976,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210.3\" y=\"-110.3\">(None, 48)</text>\n",
       "<polyline fill=\"none\" points=\"171.976,-103.5 248.625,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210.3\" y=\"-88.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 103720884320&#45;&gt;103720884544 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>103720884320-&gt;103720884544</title>\n",
       "<path d=\"M130.34,-162.329C130.34,-154.183 130.34,-144.699 130.34,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"133.84,-135.729 130.34,-125.729 126.84,-135.729 133.84,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 103720884152 -->\n",
       "<g class=\"node\" id=\"node4\"><title>103720884152</title>\n",
       "<polygon fill=\"none\" points=\"0.379395,-0.5 0.379395,-44.5 260.3,-44.5 260.3,-0.5 0.379395,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-18.3\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"127.983,-0.5 127.983,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.817\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"127.983,-22.5 183.652,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.817\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"183.652,-0.5 183.652,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.976\" y=\"-29.3\">(None, 48)</text>\n",
       "<polyline fill=\"none\" points=\"183.652,-22.5 260.3,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.976\" y=\"-7.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 103720884544&#45;&gt;103720884152 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>103720884544-&gt;103720884152</title>\n",
       "<path d=\"M130.34,-81.3294C130.34,-73.1826 130.34,-63.6991 130.34,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"133.84,-54.729 130.34,-44.729 126.84,-54.729 133.84,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 80 samples\n",
      "Epoch 1/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 34.1550 - val_loss: 67.6305\n",
      "Epoch 2/1500\n",
      "720/720 [==============================] - 0s 47us/step - loss: 34.3309 - val_loss: 68.2056\n",
      "Epoch 3/1500\n",
      "720/720 [==============================] - 0s 52us/step - loss: 34.0228 - val_loss: 69.4131\n",
      "Epoch 4/1500\n",
      "720/720 [==============================] - 0s 87us/step - loss: 33.3299 - val_loss: 69.3883\n",
      "Epoch 5/1500\n",
      "720/720 [==============================] - 0s 78us/step - loss: 33.5856 - val_loss: 68.6971\n",
      "Epoch 6/1500\n",
      "720/720 [==============================] - 0s 87us/step - loss: 33.8712 - val_loss: 68.6377\n",
      "Epoch 7/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 34.3130 - val_loss: 70.1241\n",
      "Epoch 8/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.9830 - val_loss: 68.4149\n",
      "Epoch 9/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.2324 - val_loss: 68.9505\n",
      "Epoch 10/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.5826 - val_loss: 69.3333\n",
      "Epoch 11/1500\n",
      "720/720 [==============================] - 0s 69us/step - loss: 33.4612 - val_loss: 69.2546\n",
      "Epoch 12/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.3799 - val_loss: 69.3348\n",
      "Epoch 13/1500\n",
      "720/720 [==============================] - 0s 69us/step - loss: 33.5185 - val_loss: 69.7713\n",
      "Epoch 14/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 34.1730 - val_loss: 69.4643\n",
      "Epoch 15/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 34.7177 - val_loss: 70.4733\n",
      "Epoch 16/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 34.1941 - val_loss: 68.7341\n",
      "Epoch 17/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 33.7264 - val_loss: 69.0500\n",
      "Epoch 18/1500\n",
      "720/720 [==============================] - 0s 195us/step - loss: 34.0197 - val_loss: 69.3309\n",
      "Epoch 19/1500\n",
      "720/720 [==============================] - 0s 134us/step - loss: 33.6478 - val_loss: 68.2389\n",
      "Epoch 20/1500\n",
      "720/720 [==============================] - 0s 94us/step - loss: 33.5750 - val_loss: 69.3168\n",
      "Epoch 21/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 34.0784 - val_loss: 70.1262\n",
      "Epoch 22/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 33.7473 - val_loss: 68.7457\n",
      "Epoch 23/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.6576 - val_loss: 68.1361\n",
      "Epoch 24/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 33.2091 - val_loss: 69.1515\n",
      "Epoch 25/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.6739 - val_loss: 68.4545\n",
      "Epoch 26/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.9391 - val_loss: 68.8680\n",
      "Epoch 27/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.8355 - val_loss: 69.9104\n",
      "Epoch 28/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.9995 - val_loss: 67.6229\n",
      "Epoch 29/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.7213 - val_loss: 67.9806\n",
      "Epoch 30/1500\n",
      "720/720 [==============================] - 0s 110us/step - loss: 33.9080 - val_loss: 69.8438\n",
      "Epoch 31/1500\n",
      "720/720 [==============================] - 0s 81us/step - loss: 33.5786 - val_loss: 70.0219\n",
      "Epoch 32/1500\n",
      "720/720 [==============================] - 0s 98us/step - loss: 33.8021 - val_loss: 68.2108\n",
      "Epoch 33/1500\n",
      "720/720 [==============================] - 0s 80us/step - loss: 33.4055 - val_loss: 69.2970\n",
      "Epoch 34/1500\n",
      "720/720 [==============================] - 0s 76us/step - loss: 33.4033 - val_loss: 69.0849\n",
      "Epoch 35/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.3186 - val_loss: 69.0865\n",
      "Epoch 36/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.6742 - val_loss: 68.6296\n",
      "Epoch 37/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.1745 - val_loss: 68.8137\n",
      "Epoch 38/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.4448 - val_loss: 68.2230\n",
      "Epoch 39/1500\n",
      "720/720 [==============================] - 0s 87us/step - loss: 33.6046 - val_loss: 69.5582\n",
      "Epoch 40/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.5279 - val_loss: 69.6722\n",
      "Epoch 41/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.9969 - val_loss: 68.1379\n",
      "Epoch 42/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 34.2575 - val_loss: 70.2123\n",
      "Epoch 43/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.7261 - val_loss: 68.3593\n",
      "Epoch 44/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.6848 - val_loss: 67.8716\n",
      "Epoch 45/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.3745 - val_loss: 69.6735\n",
      "Epoch 46/1500\n",
      "720/720 [==============================] - 0s 54us/step - loss: 33.9254 - val_loss: 69.6247\n",
      "Epoch 47/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.7644 - val_loss: 69.1523\n",
      "Epoch 48/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.0547 - val_loss: 69.5908\n",
      "Epoch 49/1500\n",
      "720/720 [==============================] - 0s 90us/step - loss: 33.3112 - val_loss: 69.4519\n",
      "Epoch 50/1500\n",
      "720/720 [==============================] - 0s 83us/step - loss: 33.3088 - val_loss: 69.2683\n",
      "Epoch 51/1500\n",
      "720/720 [==============================] - 0s 69us/step - loss: 33.0165 - val_loss: 69.2162\n",
      "Epoch 52/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.2762 - val_loss: 69.1190\n",
      "Epoch 53/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.2226 - val_loss: 69.6039\n",
      "Epoch 54/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.0339 - val_loss: 69.6554\n",
      "Epoch 55/1500\n",
      "720/720 [==============================] - 0s 61us/step - loss: 32.8924 - val_loss: 69.2223\n",
      "Epoch 56/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 32.8538 - val_loss: 69.7000\n",
      "Epoch 57/1500\n",
      "720/720 [==============================] - 0s 53us/step - loss: 33.2012 - val_loss: 68.5981\n",
      "Epoch 58/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.5478 - val_loss: 70.7518\n",
      "Epoch 59/1500\n",
      "720/720 [==============================] - 0s 48us/step - loss: 34.4425 - val_loss: 69.3873\n",
      "Epoch 60/1500\n",
      "720/720 [==============================] - ETA: 0s - loss: 35.94 - 0s 50us/step - loss: 34.2266 - val_loss: 68.4354\n",
      "Epoch 61/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 34.2483 - val_loss: 70.2345\n",
      "Epoch 62/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 33.4870 - val_loss: 70.0123\n",
      "Epoch 63/1500\n",
      "720/720 [==============================] - 0s 50us/step - loss: 34.1815 - val_loss: 67.9627\n",
      "Epoch 64/1500\n",
      "720/720 [==============================] - 0s 49us/step - loss: 34.3144 - val_loss: 70.7965\n",
      "Epoch 65/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.9126 - val_loss: 69.8234\n",
      "Epoch 66/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.7992 - val_loss: 70.4894\n",
      "Epoch 67/1500\n",
      "720/720 [==============================] - ETA: 0s - loss: 34.38 - 0s 97us/step - loss: 34.1845 - val_loss: 70.9571\n",
      "Epoch 68/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.4011 - val_loss: 70.4863\n",
      "Epoch 69/1500\n",
      "720/720 [==============================] - 0s 53us/step - loss: 33.5489 - val_loss: 70.4399\n",
      "Epoch 70/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 34.2791 - val_loss: 68.4010\n",
      "Epoch 71/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.7539 - val_loss: 69.0504\n",
      "Epoch 72/1500\n",
      "720/720 [==============================] - 0s 96us/step - loss: 33.4385 - val_loss: 69.3660\n",
      "Epoch 73/1500\n",
      "720/720 [==============================] - 0s 79us/step - loss: 33.5435 - val_loss: 68.4100\n",
      "Epoch 74/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.4886 - val_loss: 69.5439\n",
      "Epoch 75/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.1198 - val_loss: 68.7687\n",
      "Epoch 76/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.0797 - val_loss: 68.1920\n",
      "Epoch 77/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.1936 - val_loss: 69.2334\n",
      "Epoch 78/1500\n",
      "720/720 [==============================] - 0s 54us/step - loss: 33.4272 - val_loss: 69.0806\n",
      "Epoch 79/1500\n",
      "720/720 [==============================] - 0s 50us/step - loss: 33.4222 - val_loss: 69.6849\n",
      "Epoch 80/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 33.4189 - val_loss: 69.7739\n",
      "Epoch 81/1500\n",
      "720/720 [==============================] - 0s 46us/step - loss: 33.5679 - val_loss: 69.1301\n",
      "Epoch 82/1500\n",
      "720/720 [==============================] - 0s 49us/step - loss: 33.7291 - val_loss: 69.9990\n",
      "Epoch 83/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.7017 - val_loss: 68.6776\n",
      "Epoch 84/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.6713 - val_loss: 69.0915\n",
      "Epoch 85/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 33.5103 - val_loss: 70.0502\n",
      "Epoch 86/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 34.3030 - val_loss: 68.3699\n",
      "Epoch 87/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 33.6202 - val_loss: 69.9650\n",
      "Epoch 88/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.2476 - val_loss: 69.8276\n",
      "Epoch 89/1500\n",
      "720/720 [==============================] - ETA: 0s - loss: 30.84 - 0s 81us/step - loss: 33.5799 - val_loss: 69.0164\n",
      "Epoch 90/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.4377 - val_loss: 69.6788\n",
      "Epoch 91/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 34.3076 - val_loss: 70.5321\n",
      "Epoch 92/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 34.1887 - val_loss: 69.8300\n",
      "Epoch 93/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 34.2450 - val_loss: 70.8076\n",
      "Epoch 94/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 33.4785 - val_loss: 70.6527\n",
      "Epoch 95/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.6399 - val_loss: 69.2306\n",
      "Epoch 96/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.3603 - val_loss: 70.3857\n",
      "Epoch 97/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 34.2904 - val_loss: 70.0822\n",
      "Epoch 98/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 33.5835 - val_loss: 69.6605\n",
      "Epoch 99/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 34.0124 - val_loss: 70.1634\n",
      "Epoch 100/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.4595 - val_loss: 69.2465\n",
      "Epoch 101/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.3860 - val_loss: 69.4439\n",
      "Epoch 102/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.6417 - val_loss: 69.7719\n",
      "Epoch 103/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 34.0718 - val_loss: 72.0444\n",
      "Epoch 104/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.5606 - val_loss: 69.4513\n",
      "Epoch 105/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.3058 - val_loss: 71.3785\n",
      "Epoch 106/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.2542 - val_loss: 70.6533\n",
      "Epoch 107/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.4668 - val_loss: 70.0015\n",
      "Epoch 108/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 32.7706 - val_loss: 69.9724\n",
      "Epoch 109/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.2160 - val_loss: 69.9400\n",
      "Epoch 110/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 33.0213 - val_loss: 70.4619\n",
      "Epoch 111/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 32.5948 - val_loss: 70.0774\n",
      "Epoch 112/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 33.4811 - val_loss: 69.9595\n",
      "Epoch 113/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 33.8221 - val_loss: 69.9249\n",
      "Epoch 114/1500\n",
      "720/720 [==============================] - 0s 78us/step - loss: 33.2534 - val_loss: 70.6271\n",
      "Epoch 115/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 32.9482 - val_loss: 70.1306\n",
      "Epoch 116/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.0962 - val_loss: 68.9838\n",
      "Epoch 117/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.1822 - val_loss: 69.5368\n",
      "Epoch 118/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 34.0871 - val_loss: 69.9946\n",
      "Epoch 119/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.2154 - val_loss: 70.5223\n",
      "Epoch 120/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.3215 - val_loss: 68.8931\n",
      "Epoch 121/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.3699 - val_loss: 70.9811\n",
      "Epoch 122/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.8930 - val_loss: 70.1448\n",
      "Epoch 123/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.8148 - val_loss: 68.9825\n",
      "Epoch 124/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.2087 - val_loss: 71.6329\n",
      "Epoch 125/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.2117 - val_loss: 70.6403\n",
      "Epoch 126/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.3748 - val_loss: 70.1903\n",
      "Epoch 127/1500\n",
      "720/720 [==============================] - 0s 78us/step - loss: 34.1246 - val_loss: 69.7835\n",
      "Epoch 128/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.7250 - val_loss: 72.4919\n",
      "Epoch 129/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 33.6649 - val_loss: 70.1735\n",
      "Epoch 130/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 34.1263 - val_loss: 70.0719\n",
      "Epoch 131/1500\n",
      "720/720 [==============================] - 0s 80us/step - loss: 33.6084 - val_loss: 71.1397\n",
      "Epoch 132/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.8123 - val_loss: 69.4059\n",
      "Epoch 133/1500\n",
      "720/720 [==============================] - 0s 83us/step - loss: 32.9261 - val_loss: 69.3928\n",
      "Epoch 134/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.4350 - val_loss: 70.8995\n",
      "Epoch 135/1500\n",
      "720/720 [==============================] - 0s 81us/step - loss: 33.0341 - val_loss: 69.5494\n",
      "Epoch 136/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 34.0442 - val_loss: 70.0374\n",
      "Epoch 137/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.2681 - val_loss: 72.3578\n",
      "Epoch 138/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 33.7973 - val_loss: 71.1664\n",
      "Epoch 139/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.1557 - val_loss: 69.9990\n",
      "Epoch 140/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 33.4714 - val_loss: 69.4918\n",
      "Epoch 141/1500\n",
      "720/720 [==============================] - 0s 89us/step - loss: 34.0121 - val_loss: 70.0237\n",
      "Epoch 142/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.4900 - val_loss: 70.4235\n",
      "Epoch 143/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.5194 - val_loss: 69.7545\n",
      "Epoch 144/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.3537 - val_loss: 71.3482\n",
      "Epoch 145/1500\n",
      "720/720 [==============================] - 0s 53us/step - loss: 33.4447 - val_loss: 70.1929\n",
      "Epoch 146/1500\n",
      "720/720 [==============================] - 0s 49us/step - loss: 33.3614 - val_loss: 70.7689\n",
      "Epoch 147/1500\n",
      "720/720 [==============================] - 0s 47us/step - loss: 33.0475 - val_loss: 69.7850\n",
      "Epoch 148/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.4439 - val_loss: 69.5966\n",
      "Epoch 149/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 34.6037 - val_loss: 71.0665\n",
      "Epoch 150/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 34.4151 - val_loss: 69.1374\n",
      "Epoch 151/1500\n",
      "720/720 [==============================] - 0s 83us/step - loss: 33.7554 - val_loss: 70.5544\n",
      "Epoch 152/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.1462 - val_loss: 71.9943\n",
      "Epoch 153/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 74us/step - loss: 33.6223 - val_loss: 71.9646\n",
      "Epoch 154/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.9002 - val_loss: 69.0097\n",
      "Epoch 155/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.7809 - val_loss: 70.8974\n",
      "Epoch 156/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.5878 - val_loss: 70.1629\n",
      "Epoch 157/1500\n",
      "720/720 [==============================] - 0s 81us/step - loss: 33.4841 - val_loss: 70.4948\n",
      "Epoch 158/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.1002 - val_loss: 70.3129\n",
      "Epoch 159/1500\n",
      "720/720 [==============================] - 0s 61us/step - loss: 33.6371 - val_loss: 71.8551\n",
      "Epoch 160/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.5226 - val_loss: 69.7989\n",
      "Epoch 161/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.9675 - val_loss: 70.4248\n",
      "Epoch 162/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.5701 - val_loss: 71.0211\n",
      "Epoch 163/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.4038 - val_loss: 70.0626\n",
      "Epoch 164/1500\n",
      "720/720 [==============================] - 0s 80us/step - loss: 33.3688 - val_loss: 72.0503\n",
      "Epoch 165/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.3733 - val_loss: 69.1171\n",
      "Epoch 166/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.3678 - val_loss: 68.5114\n",
      "Epoch 167/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.6520 - val_loss: 68.5204\n",
      "Epoch 168/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.8304 - val_loss: 69.7201\n",
      "Epoch 169/1500\n",
      "720/720 [==============================] - 0s 76us/step - loss: 33.0192 - val_loss: 69.1726\n",
      "Epoch 170/1500\n",
      "720/720 [==============================] - 0s 79us/step - loss: 33.0316 - val_loss: 70.9346\n",
      "Epoch 171/1500\n",
      "720/720 [==============================] - 0s 90us/step - loss: 33.5479 - val_loss: 71.7596\n",
      "Epoch 172/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.3459 - val_loss: 70.9558\n",
      "Epoch 173/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.0019 - val_loss: 70.1870\n",
      "Epoch 174/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.0680 - val_loss: 71.2424\n",
      "Epoch 175/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.0659 - val_loss: 69.8743\n",
      "Epoch 176/1500\n",
      "720/720 [==============================] - 0s 81us/step - loss: 33.1074 - val_loss: 71.8650\n",
      "Epoch 177/1500\n",
      "720/720 [==============================] - 0s 108us/step - loss: 33.5580 - val_loss: 69.3101\n",
      "Epoch 178/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.7501 - val_loss: 71.0380\n",
      "Epoch 179/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.1777 - val_loss: 70.2378\n",
      "Epoch 180/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.1032 - val_loss: 71.0900\n",
      "Epoch 181/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.2185 - val_loss: 69.5963\n",
      "Epoch 182/1500\n",
      "720/720 [==============================] - 0s 104us/step - loss: 32.9617 - val_loss: 70.8705\n",
      "Epoch 183/1500\n",
      "720/720 [==============================] - 0s 82us/step - loss: 32.7298 - val_loss: 70.2001\n",
      "Epoch 184/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 33.2925 - val_loss: 69.8535\n",
      "Epoch 185/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.2456 - val_loss: 70.5535\n",
      "Epoch 186/1500\n",
      "720/720 [==============================] - 0s 93us/step - loss: 32.6309 - val_loss: 70.9457\n",
      "Epoch 187/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.4544 - val_loss: 69.7611\n",
      "Epoch 188/1500\n",
      "720/720 [==============================] - 0s 92us/step - loss: 33.9057 - val_loss: 70.3232\n",
      "Epoch 189/1500\n",
      "720/720 [==============================] - 0s 78us/step - loss: 33.9640 - val_loss: 70.0499\n",
      "Epoch 190/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.7030 - val_loss: 69.1357\n",
      "Epoch 191/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.5107 - val_loss: 69.9096\n",
      "Epoch 192/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.0686 - val_loss: 69.5444\n",
      "Epoch 193/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.2046 - val_loss: 70.6041\n",
      "Epoch 194/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 33.3628 - val_loss: 72.3088\n",
      "Epoch 195/1500\n",
      "720/720 [==============================] - 0s 104us/step - loss: 32.8155 - val_loss: 69.9278\n",
      "Epoch 196/1500\n",
      "720/720 [==============================] - 0s 103us/step - loss: 32.9692 - val_loss: 71.0696\n",
      "Epoch 197/1500\n",
      "720/720 [==============================] - 0s 92us/step - loss: 33.0956 - val_loss: 70.4105\n",
      "Epoch 198/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.0209 - val_loss: 70.0061\n",
      "Epoch 199/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.1983 - val_loss: 70.5501\n",
      "Epoch 200/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 32.9179 - val_loss: 70.3708\n",
      "Epoch 201/1500\n",
      "720/720 [==============================] - 0s 84us/step - loss: 33.1105 - val_loss: 71.4236\n",
      "Epoch 202/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.4931 - val_loss: 70.1711\n",
      "Epoch 203/1500\n",
      "720/720 [==============================] - 0s 96us/step - loss: 33.1389 - val_loss: 69.3859\n",
      "Epoch 204/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.1887 - val_loss: 71.0184\n",
      "Epoch 205/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.6381 - val_loss: 69.5318\n",
      "Epoch 206/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.4865 - val_loss: 69.4300\n",
      "Epoch 207/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 32.7967 - val_loss: 70.3482\n",
      "Epoch 208/1500\n",
      "720/720 [==============================] - 0s 109us/step - loss: 32.8844 - val_loss: 71.6566\n",
      "Epoch 209/1500\n",
      "720/720 [==============================] - 0s 88us/step - loss: 33.1921 - val_loss: 71.8777\n",
      "Epoch 210/1500\n",
      "720/720 [==============================] - 0s 80us/step - loss: 33.6060 - val_loss: 69.9423\n",
      "Epoch 211/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 32.9419 - val_loss: 70.1741\n",
      "Epoch 212/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.1175 - val_loss: 71.1946\n",
      "Epoch 213/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 32.4599 - val_loss: 70.5801\n",
      "Epoch 214/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 32.7724 - val_loss: 69.7045\n",
      "Epoch 215/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.1454 - val_loss: 69.7643\n",
      "Epoch 216/1500\n",
      "720/720 [==============================] - 0s 112us/step - loss: 34.0363 - val_loss: 70.7866\n",
      "Epoch 217/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.3065 - val_loss: 69.4096\n",
      "Epoch 218/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.9083 - val_loss: 71.8253\n",
      "Epoch 219/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 34.4716 - val_loss: 69.6013\n",
      "Epoch 220/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 34.0426 - val_loss: 69.9046\n",
      "Epoch 221/1500\n",
      "720/720 [==============================] - 0s 80us/step - loss: 32.7906 - val_loss: 70.8845\n",
      "Epoch 222/1500\n",
      "720/720 [==============================] - 0s 76us/step - loss: 32.9398 - val_loss: 69.2825\n",
      "Epoch 223/1500\n",
      "720/720 [==============================] - 0s 89us/step - loss: 33.2070 - val_loss: 71.7008\n",
      "Epoch 224/1500\n",
      "720/720 [==============================] - 0s 79us/step - loss: 32.8090 - val_loss: 70.3513\n",
      "Epoch 225/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.0043 - val_loss: 70.7328\n",
      "Epoch 226/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.1171 - val_loss: 71.9192\n",
      "Epoch 227/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.8097 - val_loss: 70.3534\n",
      "Epoch 228/1500\n",
      "720/720 [==============================] - 0s 90us/step - loss: 32.7907 - val_loss: 69.8315\n",
      "Epoch 229/1500\n",
      "720/720 [==============================] - 0s 84us/step - loss: 32.8210 - val_loss: 69.8167\n",
      "Epoch 230/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 32.9342 - val_loss: 69.9528\n",
      "Epoch 231/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 33.1853 - val_loss: 72.1765\n",
      "Epoch 232/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 33.5767 - val_loss: 69.6716\n",
      "Epoch 233/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.6271 - val_loss: 72.1154\n",
      "Epoch 234/1500\n",
      "720/720 [==============================] - 0s 78us/step - loss: 32.8121 - val_loss: 70.4011\n",
      "Epoch 235/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.1404 - val_loss: 71.3595\n",
      "Epoch 236/1500\n",
      "720/720 [==============================] - 0s 92us/step - loss: 32.6459 - val_loss: 69.1970\n",
      "Epoch 237/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 33.6513 - val_loss: 70.9607\n",
      "Epoch 238/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.8699 - val_loss: 70.1027\n",
      "Epoch 239/1500\n",
      "720/720 [==============================] - 0s 61us/step - loss: 33.0390 - val_loss: 70.4030\n",
      "Epoch 240/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 32.8681 - val_loss: 71.1433\n",
      "Epoch 241/1500\n",
      "720/720 [==============================] - 0s 92us/step - loss: 32.7642 - val_loss: 71.1691\n",
      "Epoch 242/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 32.6651 - val_loss: 71.5466\n",
      "Epoch 243/1500\n",
      "720/720 [==============================] - 0s 94us/step - loss: 32.4954 - val_loss: 71.3467\n",
      "Epoch 244/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 32.6725 - val_loss: 70.6671\n",
      "Epoch 245/1500\n",
      "720/720 [==============================] - 0s 53us/step - loss: 33.0346 - val_loss: 70.5810\n",
      "Epoch 246/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.6541 - val_loss: 70.6625\n",
      "Epoch 247/1500\n",
      "720/720 [==============================] - 0s 60us/step - loss: 32.4954 - val_loss: 70.9199\n",
      "Epoch 248/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 32.9364 - val_loss: 71.8074\n",
      "Epoch 249/1500\n",
      "720/720 [==============================] - 0s 108us/step - loss: 33.8838 - val_loss: 70.2850\n",
      "Epoch 250/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 34.0248 - val_loss: 70.4725\n",
      "Epoch 251/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 33.1394 - val_loss: 69.9170\n",
      "Epoch 252/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.1520 - val_loss: 70.5447\n",
      "Epoch 253/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 32.7786 - val_loss: 69.7507\n",
      "Epoch 254/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.0872 - val_loss: 71.7986\n",
      "Epoch 255/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 32.6136 - val_loss: 70.0828\n",
      "Epoch 256/1500\n",
      "720/720 [==============================] - 0s 84us/step - loss: 33.0994 - val_loss: 71.7309\n",
      "Epoch 257/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 33.4217 - val_loss: 71.3336\n",
      "Epoch 258/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.2308 - val_loss: 71.2938\n",
      "Epoch 259/1500\n",
      "720/720 [==============================] - 0s 79us/step - loss: 32.7275 - val_loss: 70.7276\n",
      "Epoch 260/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 32.8592 - val_loss: 70.2336\n",
      "Epoch 261/1500\n",
      "720/720 [==============================] - 0s 92us/step - loss: 32.9417 - val_loss: 70.2670\n",
      "Epoch 262/1500\n",
      "720/720 [==============================] - 0s 94us/step - loss: 32.9538 - val_loss: 70.4826\n",
      "Epoch 263/1500\n",
      "720/720 [==============================] - 0s 86us/step - loss: 33.0170 - val_loss: 70.7602\n",
      "Epoch 264/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.5888 - val_loss: 71.7484\n",
      "Epoch 265/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 33.0737 - val_loss: 70.4530\n",
      "Epoch 266/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 33.4971 - val_loss: 70.0686\n",
      "Epoch 267/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 33.1074 - val_loss: 70.0358\n",
      "Epoch 268/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.0279 - val_loss: 70.7370\n",
      "Epoch 269/1500\n",
      "720/720 [==============================] - 0s 90us/step - loss: 33.1475 - val_loss: 70.7793\n",
      "Epoch 270/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 32.7912 - val_loss: 70.8810\n",
      "Epoch 271/1500\n",
      "720/720 [==============================] - 0s 69us/step - loss: 33.0097 - val_loss: 70.4884\n",
      "Epoch 272/1500\n",
      "720/720 [==============================] - 0s 66us/step - loss: 32.8622 - val_loss: 70.2246\n",
      "Epoch 273/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 33.1707 - val_loss: 70.2607\n",
      "Epoch 274/1500\n",
      "720/720 [==============================] - 0s 81us/step - loss: 32.9034 - val_loss: 69.8675\n",
      "Epoch 275/1500\n",
      "720/720 [==============================] - 0s 100us/step - loss: 33.2215 - val_loss: 72.4032\n",
      "Epoch 276/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.3102 - val_loss: 70.9795\n",
      "Epoch 277/1500\n",
      "720/720 [==============================] - 0s 109us/step - loss: 33.0539 - val_loss: 72.1640\n",
      "Epoch 278/1500\n",
      "720/720 [==============================] - 0s 95us/step - loss: 33.4108 - val_loss: 71.4351\n",
      "Epoch 279/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 33.3090 - val_loss: 70.7821\n",
      "Epoch 280/1500\n",
      "720/720 [==============================] - 0s 96us/step - loss: 33.1196 - val_loss: 71.0958\n",
      "Epoch 281/1500\n",
      "720/720 [==============================] - 0s 76us/step - loss: 33.1612 - val_loss: 70.9056\n",
      "Epoch 282/1500\n",
      "720/720 [==============================] - 0s 69us/step - loss: 33.0092 - val_loss: 72.6933\n",
      "Epoch 283/1500\n",
      "720/720 [==============================] - 0s 63us/step - loss: 32.7698 - val_loss: 71.2504\n",
      "Epoch 284/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.7460 - val_loss: 69.8845\n",
      "Epoch 285/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 32.6424 - val_loss: 71.8911\n",
      "Epoch 286/1500\n",
      "720/720 [==============================] - 0s 72us/step - loss: 32.4995 - val_loss: 71.2881\n",
      "Epoch 287/1500\n",
      "720/720 [==============================] - 0s 96us/step - loss: 32.7502 - val_loss: 70.0264\n",
      "Epoch 288/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 32.5672 - val_loss: 71.1176\n",
      "Epoch 289/1500\n",
      "720/720 [==============================] - 0s 93us/step - loss: 33.2634 - val_loss: 71.2159\n",
      "Epoch 290/1500\n",
      "720/720 [==============================] - 0s 90us/step - loss: 32.6761 - val_loss: 70.1009\n",
      "Epoch 291/1500\n",
      "720/720 [==============================] - 0s 73us/step - loss: 32.7801 - val_loss: 70.5953\n",
      "Epoch 292/1500\n",
      "720/720 [==============================] - 0s 93us/step - loss: 33.4718 - val_loss: 71.0416\n",
      "Epoch 293/1500\n",
      "720/720 [==============================] - 0s 102us/step - loss: 33.7115 - val_loss: 71.9189\n",
      "Epoch 294/1500\n",
      "720/720 [==============================] - 0s 93us/step - loss: 33.6464 - val_loss: 70.4171\n",
      "Epoch 295/1500\n",
      "720/720 [==============================] - 0s 100us/step - loss: 33.3566 - val_loss: 71.3804\n",
      "Epoch 296/1500\n",
      "720/720 [==============================] - 0s 94us/step - loss: 33.1438 - val_loss: 70.9782\n",
      "Epoch 297/1500\n",
      "720/720 [==============================] - 0s 101us/step - loss: 32.8769 - val_loss: 70.8674\n",
      "Epoch 298/1500\n",
      "720/720 [==============================] - 0s 84us/step - loss: 32.5008 - val_loss: 71.0859\n",
      "Epoch 299/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 32.3945 - val_loss: 70.9209\n",
      "Epoch 300/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 32.5823 - val_loss: 69.6232\n",
      "Epoch 301/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 33.5030 - val_loss: 71.4951\n",
      "Epoch 302/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 33.2780 - val_loss: 70.1852\n",
      "Epoch 303/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 32.7797 - val_loss: 71.0619\n",
      "Epoch 304/1500\n",
      "720/720 [==============================] - 0s 61us/step - loss: 33.0237 - val_loss: 70.0593\n",
      "Epoch 305/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 58us/step - loss: 34.3991 - val_loss: 71.9235\n",
      "Epoch 306/1500\n",
      "720/720 [==============================] - 0s 50us/step - loss: 33.0165 - val_loss: 70.4827\n",
      "Epoch 307/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 33.0750 - val_loss: 71.8164\n",
      "Epoch 308/1500\n",
      "720/720 [==============================] - 0s 47us/step - loss: 33.5007 - val_loss: 70.3046\n",
      "Epoch 309/1500\n",
      "720/720 [==============================] - 0s 54us/step - loss: 32.8159 - val_loss: 71.4879\n",
      "Epoch 310/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 32.9981 - val_loss: 71.0654\n",
      "Epoch 311/1500\n",
      "720/720 [==============================] - 0s 47us/step - loss: 32.9308 - val_loss: 70.7069\n",
      "Epoch 312/1500\n",
      "720/720 [==============================] - 0s 74us/step - loss: 33.1550 - val_loss: 71.6379\n",
      "Epoch 313/1500\n",
      "720/720 [==============================] - 0s 106us/step - loss: 33.3923 - val_loss: 70.2671\n",
      "Epoch 314/1500\n",
      "720/720 [==============================] - 0s 107us/step - loss: 33.3465 - val_loss: 71.3054\n",
      "Epoch 315/1500\n",
      "720/720 [==============================] - 0s 110us/step - loss: 32.8108 - val_loss: 70.8821\n",
      "Epoch 316/1500\n",
      "720/720 [==============================] - 0s 107us/step - loss: 32.6002 - val_loss: 71.0465\n",
      "Epoch 317/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 32.5760 - val_loss: 71.4233\n",
      "Epoch 318/1500\n",
      "720/720 [==============================] - 0s 75us/step - loss: 32.3483 - val_loss: 71.7590\n",
      "Epoch 319/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.1725 - val_loss: 69.6191\n",
      "Epoch 320/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 32.5447 - val_loss: 71.4248\n",
      "Epoch 321/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 33.0326 - val_loss: 71.8889\n",
      "Epoch 322/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 32.7242 - val_loss: 69.7628\n",
      "Epoch 323/1500\n",
      "720/720 [==============================] - 0s 62us/step - loss: 32.8884 - val_loss: 70.7202\n",
      "Epoch 324/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 33.3399 - val_loss: 70.7013\n",
      "Epoch 325/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 32.9682 - val_loss: 69.9585\n",
      "Epoch 326/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 32.7256 - val_loss: 70.4571\n",
      "Epoch 327/1500\n",
      "720/720 [==============================] - 0s 57us/step - loss: 32.3696 - val_loss: 70.9235\n",
      "Epoch 328/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.6537 - val_loss: 71.3947\n",
      "Epoch 329/1500\n",
      "720/720 [==============================] - 0s 77us/step - loss: 32.4403 - val_loss: 70.8690\n",
      "Epoch 330/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 32.5557 - val_loss: 72.6035\n",
      "Epoch 331/1500\n",
      "720/720 [==============================] - 0s 65us/step - loss: 33.1919 - val_loss: 72.5327\n",
      "Epoch 332/1500\n",
      "720/720 [==============================] - 0s 67us/step - loss: 32.9941 - val_loss: 70.6774\n",
      "Epoch 333/1500\n",
      "720/720 [==============================] - 0s 71us/step - loss: 32.5409 - val_loss: 70.8340\n",
      "Epoch 334/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 32.7791 - val_loss: 70.3056\n",
      "Epoch 335/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 33.1779 - val_loss: 71.7990\n",
      "Epoch 336/1500\n",
      "720/720 [==============================] - 0s 64us/step - loss: 32.7687 - val_loss: 70.1433\n",
      "Epoch 337/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 32.9963 - val_loss: 71.6532\n",
      "Epoch 338/1500\n",
      "720/720 [==============================] - 0s 70us/step - loss: 32.5732 - val_loss: 72.1567\n",
      "Epoch 339/1500\n",
      "720/720 [==============================] - 0s 55us/step - loss: 32.8589 - val_loss: 71.0658\n",
      "Epoch 340/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 32.8803 - val_loss: 71.1336\n",
      "Epoch 341/1500\n",
      "720/720 [==============================] - 0s 56us/step - loss: 32.4050 - val_loss: 70.9926\n",
      "Epoch 342/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.3142 - val_loss: 70.3244\n",
      "Epoch 343/1500\n",
      "720/720 [==============================] - 0s 51us/step - loss: 32.9421 - val_loss: 70.5619\n",
      "Epoch 344/1500\n",
      "720/720 [==============================] - 0s 52us/step - loss: 33.0691 - val_loss: 71.2500\n",
      "Epoch 345/1500\n",
      "720/720 [==============================] - 0s 48us/step - loss: 33.2953 - val_loss: 70.2336\n",
      "Epoch 346/1500\n",
      "720/720 [==============================] - 0s 61us/step - loss: 32.8799 - val_loss: 70.9728\n",
      "Epoch 347/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 32.7068 - val_loss: 72.8550\n",
      "Epoch 348/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 32.7851 - val_loss: 69.7973\n",
      "Epoch 349/1500\n",
      "720/720 [==============================] - 0s 58us/step - loss: 33.1772 - val_loss: 71.2876\n",
      "Epoch 350/1500\n",
      "720/720 [==============================] - 0s 59us/step - loss: 32.8119 - val_loss: 71.5166\n",
      "Epoch 351/1500\n",
      "720/720 [==============================] - 0s 68us/step - loss: 32.4788 - val_loss: 71.6946\n",
      "Epoch 352/1500\n",
      "720/720 [==============================] - 0s 54us/step - loss: 33.4133 - val_loss: 71.4298\n",
      "Epoch 353/1500\n",
      " 32/720 [>.............................] - ETA: 0s - loss: 30.6343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2a4ec4041603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    113\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   4100\u001b[0m     \"\"\"\n\u001b[1;32m   4101\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 4102\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   4103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4017\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   4152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2909\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(combined[:800], seq1[:800], epochs=1500, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18275eb358>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNxTFrpMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1827648da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 901\n",
    "ax = pd.DataFrame(model.predict(combined[start:start+1])).T.plot(legend='Pred')\n",
    "pd.DataFrame(seq1[start:start+1]).T.plot(ax=ax, legend='GT')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x183ac67198>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183acf8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model.get_weights()[0].reshape(10, 1)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
