{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sequence(start=0, length =80, p=0.95):\n",
    "    sequence = np.zeros(length)\n",
    "    sequence[0] = start\n",
    "    for i in range(1, length):\n",
    "        random_num = np.random.rand()\n",
    "        if random_num > p:\n",
    "            # Switch state\n",
    "            sequence[i] = 1-sequence[i-1]\n",
    "        else:\n",
    "            sequence[i] = sequence[i-1]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "source_1_range = [100, 200]\n",
    "source_2_range = [1000, 1300]\n",
    "\n",
    "sequence_length = 48\n",
    "num_sequences = 5000\n",
    "\n",
    "seq1 = []\n",
    "seq2 = []\n",
    "combined = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for seq in range(num_sequences):\n",
    "    if seq % (num_sequences/10)==0:\n",
    "        print(seq)\n",
    "    source_1_power_val = np.random.choice(list(range(*source_1_range)))\n",
    "    source_2_power_val = np.random.choice(list(range(*source_2_range)))\n",
    "    \n",
    "    source_1_seq = source_1_power_val*gen_sequence(start=np.random.choice([0, 1]), length =sequence_length, p=0.95)\n",
    "    source_2_seq = source_2_power_val*gen_sequence(start=np.random.choice([0, 1]), length =sequence_length, p=0.95)\n",
    "    combined_seq = source_1_seq + source_2_seq\n",
    "    seq1.append(source_1_seq)\n",
    "    seq2.append(source_2_seq)\n",
    "    combined.append(combined_seq)\n",
    "\n",
    "combined = np.array(combined)\n",
    "seq1 = np.array(seq1)\n",
    "seq2 = np.array(seq2)\n",
    "\n",
    "#seq1 = seq1.reshape(num_sequences, sequence_length, 1)\n",
    "#seq2 = seq2.reshape(num_sequences, sequence_length, 1)\n",
    "combined = combined.reshape(num_sequences, sequence_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18404b2080>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18345e9c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.convolve(seq1[0], [-1, 1])).plot()\n",
    "pd.Series(seq1[0]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq1 = seq1.reshape(num_sequences, sequence_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x182c4d6198>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD9CAYAAABazssqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182c4d1f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(combined).head(10).T.plot(legend=False, color='k',alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dense, Flatten, MaxPool1D, InputLayer, Activation, Dropout, MaxPooling1D\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        (None, 48, 1)             0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv1D)                  (None, 48, 20)            220       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 24, 20)            0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 24, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 24, 25)            2525      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 12, 25)            0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 12, 25)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 12, 30)            2280      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 6, 30)             0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 6, 30)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 6, 35)             2135      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 3, 35)             0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 3, 35)             0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 48)                5088      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 12,248\n",
      "Trainable params: 12,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "filters=20\n",
    "kernel_size=10\n",
    "model.add(InputLayer(input_shape=(sequence_length,1)))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1 ,name='C1'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Conv1D(filters=25,\n",
    "                 kernel_size=5,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1 ))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Conv1D(filters=30,\n",
    "                 kernel_size=3,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1 ))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Conv1D(filters=35,\n",
    "                 kernel_size=2,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1 ))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(sequence_length, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile('adam','mean_absolute_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 48, 1)             0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv1D)                  (None, 24, 1)             3         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 48)                624       \n",
      "=================================================================\n",
      "Total params: 627\n",
      "Trainable params: 627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "filters=1\n",
    "kernel_size=2\n",
    "model.add(InputLayer(input_shape=(sequence_length,1)))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=2 ,name='C1'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(sequence_length, activation='relu'))\n",
    "#model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile('adam','mean_absolute_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1349pt\" viewBox=\"0.00 0.00 379.93 1349.00\" width=\"380pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1345)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1345 375.928,-1345 375.928,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 104118118944 -->\n",
       "<g class=\"node\" id=\"node1\"><title>104118118944</title>\n",
       "<polygon fill=\"none\" points=\"45.124,-1296.5 45.124,-1340.5 326.804,-1340.5 326.804,-1296.5 45.124,-1296.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-1314.3\">input_40: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"180.486,-1296.5 180.486,-1340.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208.321\" y=\"-1325.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"180.486,-1318.5 236.155,-1318.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208.321\" y=\"-1303.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"236.155,-1296.5 236.155,-1340.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.479\" y=\"-1325.3\">(None, 48, 1)</text>\n",
       "<polyline fill=\"none\" points=\"236.155,-1318.5 326.804,-1318.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.479\" y=\"-1303.3\">(None, 48, 1)</text>\n",
       "</g>\n",
       "<!-- 104117329992 -->\n",
       "<g class=\"node\" id=\"node2\"><title>104117329992</title>\n",
       "<polygon fill=\"none\" points=\"65.7173,-1215.5 65.7173,-1259.5 306.21,-1259.5 306.21,-1215.5 65.7173,-1215.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-1233.3\">C1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"152.893,-1215.5 152.893,-1259.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.728\" y=\"-1244.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"152.893,-1237.5 208.562,-1237.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180.728\" y=\"-1222.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"208.562,-1215.5 208.562,-1259.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257.386\" y=\"-1244.3\">(None, 48, 1)</text>\n",
       "<polyline fill=\"none\" points=\"208.562,-1237.5 306.21,-1237.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257.386\" y=\"-1222.3\">(None, 48, 20)</text>\n",
       "</g>\n",
       "<!-- 104118118944&#45;&gt;104117329992 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>104118118944-&gt;104117329992</title>\n",
       "<path d=\"M185.964,-1296.33C185.964,-1288.18 185.964,-1278.7 185.964,-1269.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-1269.73 185.964,-1259.73 182.464,-1269.73 189.464,-1269.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118139256 -->\n",
       "<g class=\"node\" id=\"node3\"><title>104118139256</title>\n",
       "<polygon fill=\"none\" points=\"0,-1134.5 0,-1178.5 371.928,-1178.5 371.928,-1134.5 0,-1134.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-1152.3\">max_pooling1d_19: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-1134.5 218.61,-1178.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-1163.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-1156.5 274.279,-1156.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-1141.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-1134.5 274.279,-1178.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-1163.3\">(None, 48, 20)</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-1156.5 371.928,-1156.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-1141.3\">(None, 24, 20)</text>\n",
       "</g>\n",
       "<!-- 104117329992&#45;&gt;104118139256 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>104117329992-&gt;104118139256</title>\n",
       "<path d=\"M185.964,-1215.33C185.964,-1207.18 185.964,-1197.7 185.964,-1188.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-1188.73 185.964,-1178.73 182.464,-1188.73 189.464,-1188.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118119448 -->\n",
       "<g class=\"node\" id=\"node4\"><title>104118119448</title>\n",
       "<polygon fill=\"none\" points=\"42.0034,-1053.5 42.0034,-1097.5 329.924,-1097.5 329.924,-1053.5 42.0034,-1053.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-1071.3\">dropout_58: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"176.607,-1053.5 176.607,-1097.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.441\" y=\"-1082.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"176.607,-1075.5 232.276,-1075.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.441\" y=\"-1060.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"232.276,-1053.5 232.276,-1097.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-1082.3\">(None, 24, 20)</text>\n",
       "<polyline fill=\"none\" points=\"232.276,-1075.5 329.924,-1075.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-1060.3\">(None, 24, 20)</text>\n",
       "</g>\n",
       "<!-- 104118139256&#45;&gt;104118119448 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>104118139256-&gt;104118119448</title>\n",
       "<path d=\"M185.964,-1134.33C185.964,-1126.18 185.964,-1116.7 185.964,-1107.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-1107.73 185.964,-1097.73 182.464,-1107.73 189.464,-1107.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118119728 -->\n",
       "<g class=\"node\" id=\"node5\"><title>104118119728</title>\n",
       "<polygon fill=\"none\" points=\"42.7793,-972.5 42.7793,-1016.5 329.148,-1016.5 329.148,-972.5 42.7793,-972.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-990.3\">conv1d_53: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"175.831,-972.5 175.831,-1016.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.666\" y=\"-1001.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"175.831,-994.5 231.5,-994.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.666\" y=\"-979.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-972.5 231.5,-1016.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-1001.3\">(None, 24, 20)</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-994.5 329.148,-994.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-979.3\">(None, 24, 25)</text>\n",
       "</g>\n",
       "<!-- 104118119448&#45;&gt;104118119728 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>104118119448-&gt;104118119728</title>\n",
       "<path d=\"M185.964,-1053.33C185.964,-1045.18 185.964,-1035.7 185.964,-1026.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-1026.73 185.964,-1016.73 182.464,-1026.73 189.464,-1026.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118120288 -->\n",
       "<g class=\"node\" id=\"node6\"><title>104118120288</title>\n",
       "<polygon fill=\"none\" points=\"0,-891.5 0,-935.5 371.928,-935.5 371.928,-891.5 0,-891.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-909.3\">max_pooling1d_20: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-891.5 218.61,-935.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-920.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-913.5 274.279,-913.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-898.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-891.5 274.279,-935.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-920.3\">(None, 24, 25)</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-913.5 371.928,-913.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-898.3\">(None, 12, 25)</text>\n",
       "</g>\n",
       "<!-- 104118119728&#45;&gt;104118120288 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>104118119728-&gt;104118120288</title>\n",
       "<path d=\"M185.964,-972.329C185.964,-964.183 185.964,-954.699 185.964,-945.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-945.729 185.964,-935.729 182.464,-945.729 189.464,-945.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118139312 -->\n",
       "<g class=\"node\" id=\"node7\"><title>104118139312</title>\n",
       "<polygon fill=\"none\" points=\"42.0034,-810.5 42.0034,-854.5 329.924,-854.5 329.924,-810.5 42.0034,-810.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-828.3\">dropout_59: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"176.607,-810.5 176.607,-854.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.441\" y=\"-839.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"176.607,-832.5 232.276,-832.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.441\" y=\"-817.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"232.276,-810.5 232.276,-854.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-839.3\">(None, 12, 25)</text>\n",
       "<polyline fill=\"none\" points=\"232.276,-832.5 329.924,-832.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-817.3\">(None, 12, 25)</text>\n",
       "</g>\n",
       "<!-- 104118120288&#45;&gt;104118139312 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>104118120288-&gt;104118139312</title>\n",
       "<path d=\"M185.964,-891.329C185.964,-883.183 185.964,-873.699 185.964,-864.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-864.729 185.964,-854.729 182.464,-864.729 189.464,-864.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118014304 -->\n",
       "<g class=\"node\" id=\"node8\"><title>104118014304</title>\n",
       "<polygon fill=\"none\" points=\"42.7793,-729.5 42.7793,-773.5 329.148,-773.5 329.148,-729.5 42.7793,-729.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-747.3\">conv1d_54: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"175.831,-729.5 175.831,-773.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.666\" y=\"-758.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"175.831,-751.5 231.5,-751.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203.666\" y=\"-736.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-729.5 231.5,-773.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-758.3\">(None, 12, 25)</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-751.5 329.148,-751.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-736.3\">(None, 12, 30)</text>\n",
       "</g>\n",
       "<!-- 104118139312&#45;&gt;104118014304 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>104118139312-&gt;104118014304</title>\n",
       "<path d=\"M185.964,-810.329C185.964,-802.183 185.964,-792.699 185.964,-783.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-783.729 185.964,-773.729 182.464,-783.729 189.464,-783.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118119224 -->\n",
       "<g class=\"node\" id=\"node9\"><title>104118119224</title>\n",
       "<polygon fill=\"none\" points=\"0,-648.5 0,-692.5 371.928,-692.5 371.928,-648.5 0,-648.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109.305\" y=\"-666.3\">max_pooling1d_21: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-648.5 218.61,-692.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-677.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"218.61,-670.5 274.279,-670.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.445\" y=\"-655.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-648.5 274.279,-692.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-677.3\">(None, 12, 30)</text>\n",
       "<polyline fill=\"none\" points=\"274.279,-670.5 371.928,-670.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-655.3\">(None, 6, 30)</text>\n",
       "</g>\n",
       "<!-- 104118014304&#45;&gt;104118119224 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>104118014304-&gt;104118119224</title>\n",
       "<path d=\"M185.964,-729.329C185.964,-721.183 185.964,-711.699 185.964,-702.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-702.729 185.964,-692.729 182.464,-702.729 189.464,-702.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104117649480 -->\n",
       "<g class=\"node\" id=\"node10\"><title>104117649480</title>\n",
       "<polygon fill=\"none\" points=\"45.5034,-567.5 45.5034,-611.5 326.424,-611.5 326.424,-567.5 45.5034,-567.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-585.3\">dropout_60: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"180.107,-567.5 180.107,-611.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.941\" y=\"-596.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"180.107,-589.5 235.776,-589.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.941\" y=\"-574.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"235.776,-567.5 235.776,-611.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-596.3\">(None, 6, 30)</text>\n",
       "<polyline fill=\"none\" points=\"235.776,-589.5 326.424,-589.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-574.3\">(None, 6, 30)</text>\n",
       "</g>\n",
       "<!-- 104118119224&#45;&gt;104117649480 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>104118119224-&gt;104117649480</title>\n",
       "<path d=\"M185.964,-648.329C185.964,-640.183 185.964,-630.699 185.964,-621.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-621.729 185.964,-611.729 182.464,-621.729 189.464,-621.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104117960040 -->\n",
       "<g class=\"node\" id=\"node11\"><title>104117960040</title>\n",
       "<polygon fill=\"none\" points=\"46.2793,-486.5 46.2793,-530.5 325.648,-530.5 325.648,-486.5 46.2793,-486.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-504.3\">conv1d_55: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"179.331,-486.5 179.331,-530.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.166\" y=\"-515.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"179.331,-508.5 235,-508.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.166\" y=\"-493.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"235,-486.5 235,-530.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-515.3\">(None, 6, 30)</text>\n",
       "<polyline fill=\"none\" points=\"235,-508.5 325.648,-508.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.324\" y=\"-493.3\">(None, 6, 35)</text>\n",
       "</g>\n",
       "<!-- 104117649480&#45;&gt;104117960040 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>104117649480-&gt;104117960040</title>\n",
       "<path d=\"M185.964,-567.329C185.964,-559.183 185.964,-549.699 185.964,-540.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-540.729 185.964,-530.729 182.464,-540.729 189.464,-540.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104118159736 -->\n",
       "<g class=\"node\" id=\"node12\"><title>104118159736</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-405.5 3.5,-449.5 368.428,-449.5 368.428,-405.5 3.5,-405.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-423.3\">max_pooling1d_22: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"222.11,-405.5 222.11,-449.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.945\" y=\"-434.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"222.11,-427.5 277.779,-427.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.945\" y=\"-412.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"277.779,-405.5 277.779,-449.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-434.3\">(None, 6, 35)</text>\n",
       "<polyline fill=\"none\" points=\"277.779,-427.5 368.428,-427.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.104\" y=\"-412.3\">(None, 3, 35)</text>\n",
       "</g>\n",
       "<!-- 104117960040&#45;&gt;104118159736 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>104117960040-&gt;104118159736</title>\n",
       "<path d=\"M185.964,-486.329C185.964,-478.183 185.964,-468.699 185.964,-459.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-459.729 185.964,-449.729 182.464,-459.729 189.464,-459.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104020014192 -->\n",
       "<g class=\"node\" id=\"node13\"><title>104020014192</title>\n",
       "<polygon fill=\"none\" points=\"45.5034,-324.5 45.5034,-368.5 326.424,-368.5 326.424,-324.5 45.5034,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-342.3\">dropout_61: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"180.107,-324.5 180.107,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.941\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"180.107,-346.5 235.776,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.941\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"235.776,-324.5 235.776,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-353.3\">(None, 3, 35)</text>\n",
       "<polyline fill=\"none\" points=\"235.776,-346.5 326.424,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-331.3\">(None, 3, 35)</text>\n",
       "</g>\n",
       "<!-- 104118159736&#45;&gt;104020014192 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>104118159736-&gt;104020014192</title>\n",
       "<path d=\"M185.964,-405.329C185.964,-397.183 185.964,-387.699 185.964,-378.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-378.729 185.964,-368.729 182.464,-378.729 189.464,-378.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104019773480 -->\n",
       "<g class=\"node\" id=\"node14\"><title>104019773480</title>\n",
       "<polygon fill=\"none\" points=\"53.6724,-243.5 53.6724,-287.5 318.255,-287.5 318.255,-243.5 53.6724,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.805\" y=\"-261.3\">flatten_32: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"171.938,-243.5 171.938,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.772\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"171.938,-265.5 227.607,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.772\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"227.607,-243.5 227.607,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.931\" y=\"-272.3\">(None, 3, 35)</text>\n",
       "<polyline fill=\"none\" points=\"227.607,-265.5 318.255,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.931\" y=\"-250.3\">(None, 105)</text>\n",
       "</g>\n",
       "<!-- 104020014192&#45;&gt;104019773480 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>104020014192-&gt;104019773480</title>\n",
       "<path d=\"M185.964,-324.329C185.964,-316.183 185.964,-306.699 185.964,-297.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-297.729 185.964,-287.729 182.464,-297.729 189.464,-297.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104019803664 -->\n",
       "<g class=\"node\" id=\"node15\"><title>104019803664</title>\n",
       "<polygon fill=\"none\" points=\"37.3447,-162.5 37.3447,-206.5 334.583,-206.5 334.583,-162.5 37.3447,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.305\" y=\"-180.3\">activation_26: Activation</text>\n",
       "<polyline fill=\"none\" points=\"195.266,-162.5 195.266,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.1\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"195.266,-184.5 250.935,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.1\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"250.935,-162.5 250.935,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"292.759\" y=\"-191.3\">(None, 105)</text>\n",
       "<polyline fill=\"none\" points=\"250.935,-184.5 334.583,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"292.759\" y=\"-169.3\">(None, 105)</text>\n",
       "</g>\n",
       "<!-- 104019773480&#45;&gt;104019803664 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>104019773480-&gt;104019803664</title>\n",
       "<path d=\"M185.964,-243.329C185.964,-235.183 185.964,-225.699 185.964,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-216.729 185.964,-206.729 182.464,-216.729 189.464,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104019800192 -->\n",
       "<g class=\"node\" id=\"node16\"><title>104019800192</title>\n",
       "<polygon fill=\"none\" points=\"60.6792,-81.5 60.6792,-125.5 311.249,-125.5 311.249,-81.5 60.6792,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.305\" y=\"-99.3\">dense_30: Dense</text>\n",
       "<polyline fill=\"none\" points=\"171.931,-81.5 171.931,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.766\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"171.931,-103.5 227.6,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.766\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"227.6,-81.5 227.6,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.424\" y=\"-110.3\">(None, 105)</text>\n",
       "<polyline fill=\"none\" points=\"227.6,-103.5 311.249,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.424\" y=\"-88.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 104019803664&#45;&gt;104019800192 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>104019803664-&gt;104019800192</title>\n",
       "<path d=\"M185.964,-162.329C185.964,-154.183 185.964,-144.699 185.964,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-135.729 185.964,-125.729 182.464,-135.729 189.464,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 104117714672 -->\n",
       "<g class=\"node\" id=\"node17\"><title>104117714672</title>\n",
       "<polygon fill=\"none\" points=\"52.5034,-0.5 52.5034,-44.5 319.424,-44.5 319.424,-0.5 52.5034,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"119.805\" y=\"-18.3\">dropout_62: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"187.107,-0.5 187.107,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.941\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"187.107,-22.5 242.776,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.941\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"242.776,-0.5 242.776,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-29.3\">(None, 48)</text>\n",
       "<polyline fill=\"none\" points=\"242.776,-22.5 319.424,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.1\" y=\"-7.3\">(None, 48)</text>\n",
       "</g>\n",
       "<!-- 104019800192&#45;&gt;104117714672 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>104019800192-&gt;104117714672</title>\n",
       "<path d=\"M185.964,-81.3294C185.964,-73.1826 185.964,-63.6991 185.964,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.464,-54.729 185.964,-44.729 182.464,-54.729 189.464,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3600 samples, validate on 400 samples\n",
      "Epoch 1/1500\n",
      "3600/3600 [==============================] - 2s 536us/step - loss: 74.8175 - val_loss: 72.4977\n",
      "Epoch 2/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 71.8617 - val_loss: 64.8445\n",
      "Epoch 3/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 63.3824 - val_loss: 57.8250\n",
      "Epoch 4/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 58.7185 - val_loss: 55.6624\n",
      "Epoch 5/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 57.2508 - val_loss: 54.3763\n",
      "Epoch 6/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 55.4706 - val_loss: 54.3087\n",
      "Epoch 7/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 54.6669 - val_loss: 52.9669\n",
      "Epoch 8/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 53.9649 - val_loss: 52.6573\n",
      "Epoch 9/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 53.1902 - val_loss: 52.0303\n",
      "Epoch 10/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 52.0689 - val_loss: 50.5715\n",
      "Epoch 11/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 51.8141 - val_loss: 51.5197\n",
      "Epoch 12/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 51.2314 - val_loss: 49.5194\n",
      "Epoch 13/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 50.7799 - val_loss: 50.6330\n",
      "Epoch 14/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 50.1630 - val_loss: 49.4644\n",
      "Epoch 15/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 50.0488 - val_loss: 49.9555\n",
      "Epoch 16/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 49.2372 - val_loss: 50.4679\n",
      "Epoch 17/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 48.8327 - val_loss: 47.5328\n",
      "Epoch 18/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 48.0403 - val_loss: 46.6009\n",
      "Epoch 19/1500\n",
      "3600/3600 [==============================] - 1s 186us/step - loss: 46.8444 - val_loss: 46.9443\n",
      "Epoch 20/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 46.4476 - val_loss: 46.8572\n",
      "Epoch 21/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 45.4249 - val_loss: 41.7636\n",
      "Epoch 22/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 44.4041 - val_loss: 45.6099\n",
      "Epoch 23/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 43.6349 - val_loss: 41.1490\n",
      "Epoch 24/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 43.0899 - val_loss: 41.3468\n",
      "Epoch 25/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 42.5527 - val_loss: 41.7107\n",
      "Epoch 26/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 42.5349 - val_loss: 41.1714\n",
      "Epoch 27/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 42.2709 - val_loss: 41.7753\n",
      "Epoch 28/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 41.4768 - val_loss: 39.5648\n",
      "Epoch 29/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 40.6344 - val_loss: 38.5861\n",
      "Epoch 30/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 40.2418 - val_loss: 35.2972\n",
      "Epoch 31/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 39.6461 - val_loss: 39.0290\n",
      "Epoch 32/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 39.3931 - val_loss: 36.7376\n",
      "Epoch 33/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 39.5411 - val_loss: 38.0505\n",
      "Epoch 34/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 39.1769 - val_loss: 40.4776\n",
      "Epoch 35/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 38.7064 - val_loss: 38.4459\n",
      "Epoch 36/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 39.0181 - val_loss: 38.2913\n",
      "Epoch 37/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 37.8995 - val_loss: 37.7846\n",
      "Epoch 38/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 38.4491 - val_loss: 37.1505\n",
      "Epoch 39/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 37.6800 - val_loss: 36.8171\n",
      "Epoch 40/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 37.3127 - val_loss: 36.9642\n",
      "Epoch 41/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 37.2639 - val_loss: 38.6812\n",
      "Epoch 42/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 37.4114 - val_loss: 36.4289\n",
      "Epoch 43/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 36.8089 - val_loss: 36.0281\n",
      "Epoch 44/1500\n",
      "3600/3600 [==============================] - 1s 156us/step - loss: 36.5831 - val_loss: 35.9702\n",
      "Epoch 45/1500\n",
      "3600/3600 [==============================] - 1s 156us/step - loss: 36.7199 - val_loss: 36.3728\n",
      "Epoch 46/1500\n",
      "3600/3600 [==============================] - 1s 155us/step - loss: 36.2482 - val_loss: 36.4738\n",
      "Epoch 47/1500\n",
      "3600/3600 [==============================] - 1s 156us/step - loss: 35.9772 - val_loss: 35.2737\n",
      "Epoch 48/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 35.8099 - val_loss: 33.2502\n",
      "Epoch 49/1500\n",
      "3600/3600 [==============================] - 1s 152us/step - loss: 36.2467 - val_loss: 36.1050\n",
      "Epoch 50/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 36.3558 - val_loss: 35.9184\n",
      "Epoch 51/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 35.2015 - val_loss: 32.4338\n",
      "Epoch 52/1500\n",
      "3600/3600 [==============================] - 1s 156us/step - loss: 34.2985 - val_loss: 31.5032\n",
      "Epoch 53/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 34.6096 - val_loss: 35.4645\n",
      "Epoch 54/1500\n",
      "3600/3600 [==============================] - 1s 179us/step - loss: 34.0752 - val_loss: 33.2202\n",
      "Epoch 55/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 33.9317 - val_loss: 36.4564\n",
      "Epoch 56/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 33.6710 - val_loss: 35.1927\n",
      "Epoch 57/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 34.0443 - val_loss: 33.2795\n",
      "Epoch 58/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 33.3102 - val_loss: 32.5099\n",
      "Epoch 59/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 33.6547 - val_loss: 31.4446\n",
      "Epoch 60/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 33.2236 - val_loss: 32.2177\n",
      "Epoch 61/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 32.9717 - val_loss: 32.7731\n",
      "Epoch 62/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 33.1768 - val_loss: 30.8032\n",
      "Epoch 63/1500\n",
      "3600/3600 [==============================] - 1s 155us/step - loss: 32.9419 - val_loss: 32.1444\n",
      "Epoch 64/1500\n",
      "3600/3600 [==============================] - 1s 212us/step - loss: 32.9412 - val_loss: 34.3540\n",
      "Epoch 65/1500\n",
      "3600/3600 [==============================] - 1s 190us/step - loss: 32.5276 - val_loss: 33.1843\n",
      "Epoch 66/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 32.5556 - val_loss: 32.2646\n",
      "Epoch 67/1500\n",
      "3600/3600 [==============================] - 1s 196us/step - loss: 32.3101 - val_loss: 31.9437\n",
      "Epoch 68/1500\n",
      "3600/3600 [==============================] - 1s 198us/step - loss: 32.2212 - val_loss: 34.2447\n",
      "Epoch 69/1500\n",
      "3600/3600 [==============================] - 1s 185us/step - loss: 32.2312 - val_loss: 31.0068\n",
      "Epoch 70/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 32.7560 - val_loss: 33.5363\n",
      "Epoch 71/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 32.1321 - val_loss: 32.2493\n",
      "Epoch 72/1500\n",
      "3600/3600 [==============================] - 1s 176us/step - loss: 31.6365 - val_loss: 33.6737\n",
      "Epoch 73/1500\n",
      "3600/3600 [==============================] - 1s 179us/step - loss: 31.6986 - val_loss: 32.5569\n",
      "Epoch 74/1500\n",
      "3600/3600 [==============================] - 1s 196us/step - loss: 31.7241 - val_loss: 30.5724\n",
      "Epoch 75/1500\n",
      "3600/3600 [==============================] - 1s 156us/step - loss: 31.8171 - val_loss: 31.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 31.5861 - val_loss: 32.6164\n",
      "Epoch 77/1500\n",
      "3600/3600 [==============================] - 1s 193us/step - loss: 31.4941 - val_loss: 32.6513\n",
      "Epoch 78/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 31.4622 - val_loss: 30.0735\n",
      "Epoch 79/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 31.3837 - val_loss: 32.1874\n",
      "Epoch 80/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 30.9593 - val_loss: 32.4597\n",
      "Epoch 81/1500\n",
      "3600/3600 [==============================] - 1s 184us/step - loss: 30.7956 - val_loss: 29.9860\n",
      "Epoch 82/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 31.2304 - val_loss: 31.3158\n",
      "Epoch 83/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 30.9370 - val_loss: 29.9918\n",
      "Epoch 84/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 30.8777 - val_loss: 31.8305\n",
      "Epoch 85/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 31.0395 - val_loss: 32.7554\n",
      "Epoch 86/1500\n",
      "3600/3600 [==============================] - 1s 187us/step - loss: 30.7926 - val_loss: 30.3288\n",
      "Epoch 87/1500\n",
      "3600/3600 [==============================] - 1s 214us/step - loss: 30.3759 - val_loss: 31.5773\n",
      "Epoch 88/1500\n",
      "3600/3600 [==============================] - 1s 203us/step - loss: 30.1340 - val_loss: 30.3610\n",
      "Epoch 89/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 30.2702 - val_loss: 30.1724\n",
      "Epoch 90/1500\n",
      "3600/3600 [==============================] - 1s 224us/step - loss: 30.5737 - val_loss: 30.8216\n",
      "Epoch 91/1500\n",
      "3600/3600 [==============================] - 1s 204us/step - loss: 30.6352 - val_loss: 30.7444\n",
      "Epoch 92/1500\n",
      "3600/3600 [==============================] - 1s 262us/step - loss: 30.2059 - val_loss: 31.8076\n",
      "Epoch 93/1500\n",
      "3600/3600 [==============================] - 1s 241us/step - loss: 30.0330 - val_loss: 31.4948\n",
      "Epoch 94/1500\n",
      "3600/3600 [==============================] - 1s 182us/step - loss: 30.0278 - val_loss: 28.7460\n",
      "Epoch 95/1500\n",
      "3600/3600 [==============================] - 1s 177us/step - loss: 30.1031 - val_loss: 29.2151\n",
      "Epoch 96/1500\n",
      "3600/3600 [==============================] - 1s 178us/step - loss: 29.8585 - val_loss: 30.6103\n",
      "Epoch 97/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 30.2232 - val_loss: 32.1996\n",
      "Epoch 98/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 29.8638 - val_loss: 28.6414\n",
      "Epoch 99/1500\n",
      "3600/3600 [==============================] - 1s 197us/step - loss: 29.8493 - val_loss: 29.9994\n",
      "Epoch 100/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 29.4331 - val_loss: 30.3114\n",
      "Epoch 101/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 29.6124 - val_loss: 28.6704\n",
      "Epoch 102/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 29.9881 - val_loss: 29.7343\n",
      "Epoch 103/1500\n",
      "3600/3600 [==============================] - 1s 243us/step - loss: 30.8069 - val_loss: 28.7966\n",
      "Epoch 104/1500\n",
      "3600/3600 [==============================] - 1s 186us/step - loss: 29.7002 - val_loss: 31.3279\n",
      "Epoch 105/1500\n",
      "3600/3600 [==============================] - 1s 206us/step - loss: 29.1440 - val_loss: 28.6620\n",
      "Epoch 106/1500\n",
      "3600/3600 [==============================] - 1s 231us/step - loss: 29.0176 - val_loss: 28.8380\n",
      "Epoch 107/1500\n",
      "3600/3600 [==============================] - 1s 203us/step - loss: 29.5384 - val_loss: 28.2494\n",
      "Epoch 108/1500\n",
      "3600/3600 [==============================] - 1s 179us/step - loss: 29.2089 - val_loss: 29.3930\n",
      "Epoch 109/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 29.5124 - val_loss: 29.5580\n",
      "Epoch 110/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 29.3964 - val_loss: 28.7795\n",
      "Epoch 111/1500\n",
      "3600/3600 [==============================] - 1s 270us/step - loss: 29.3566 - val_loss: 28.7622\n",
      "Epoch 112/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 29.2814 - val_loss: 31.7169\n",
      "Epoch 113/1500\n",
      "3600/3600 [==============================] - 1s 194us/step - loss: 29.6279 - val_loss: 28.8148\n",
      "Epoch 114/1500\n",
      "3600/3600 [==============================] - 1s 193us/step - loss: 29.0583 - val_loss: 31.4731\n",
      "Epoch 115/1500\n",
      "3600/3600 [==============================] - 1s 192us/step - loss: 29.4706 - val_loss: 30.4847\n",
      "Epoch 116/1500\n",
      "3600/3600 [==============================] - 1s 184us/step - loss: 28.8995 - val_loss: 30.2814\n",
      "Epoch 117/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 29.0809 - val_loss: 29.7039\n",
      "Epoch 118/1500\n",
      "3600/3600 [==============================] - 1s 178us/step - loss: 29.0305 - val_loss: 28.7185\n",
      "Epoch 119/1500\n",
      "3600/3600 [==============================] - 1s 211us/step - loss: 28.8921 - val_loss: 28.9621\n",
      "Epoch 120/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 28.5782 - val_loss: 30.4630\n",
      "Epoch 121/1500\n",
      "3600/3600 [==============================] - 1s 199us/step - loss: 28.8774 - val_loss: 29.8472\n",
      "Epoch 122/1500\n",
      "3600/3600 [==============================] - 1s 178us/step - loss: 28.8247 - val_loss: 27.5254\n",
      "Epoch 123/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 28.7559 - val_loss: 29.0638\n",
      "Epoch 124/1500\n",
      "3600/3600 [==============================] - 1s 224us/step - loss: 28.6641 - val_loss: 30.0341\n",
      "Epoch 125/1500\n",
      "3600/3600 [==============================] - 1s 186us/step - loss: 28.6682 - val_loss: 27.3754\n",
      "Epoch 126/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 28.7786 - val_loss: 30.5584\n",
      "Epoch 127/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 29.2247 - val_loss: 30.6879\n",
      "Epoch 128/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 29.0966 - val_loss: 29.1420\n",
      "Epoch 129/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 28.3216 - val_loss: 26.9153\n",
      "Epoch 130/1500\n",
      "3600/3600 [==============================] - 1s 202us/step - loss: 28.3760 - val_loss: 28.7250\n",
      "Epoch 131/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 28.3399 - val_loss: 27.7014\n",
      "Epoch 132/1500\n",
      "3600/3600 [==============================] - 1s 189us/step - loss: 28.4016 - val_loss: 28.3410\n",
      "Epoch 133/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 28.3897 - val_loss: 29.1336\n",
      "Epoch 134/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 29.8176 - val_loss: 28.9273\n",
      "Epoch 135/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 28.7682 - val_loss: 25.9000\n",
      "Epoch 136/1500\n",
      "3600/3600 [==============================] - 1s 182us/step - loss: 28.2732 - val_loss: 30.6246\n",
      "Epoch 137/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 28.3025 - val_loss: 28.9432\n",
      "Epoch 138/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 28.8451 - val_loss: 28.3192\n",
      "Epoch 139/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 28.2004 - val_loss: 27.9550\n",
      "Epoch 140/1500\n",
      "3600/3600 [==============================] - 1s 225us/step - loss: 28.2761 - val_loss: 27.5800\n",
      "Epoch 141/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 27.9873 - val_loss: 28.1843\n",
      "Epoch 142/1500\n",
      "3600/3600 [==============================] - 1s 195us/step - loss: 28.1130 - val_loss: 27.5737\n",
      "Epoch 143/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 28.0480 - val_loss: 32.2411\n",
      "Epoch 144/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 28.1549 - val_loss: 28.5353\n",
      "Epoch 145/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 28.4108 - val_loss: 29.6842\n",
      "Epoch 146/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 28.2737 - val_loss: 28.8981\n",
      "Epoch 147/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 27.7077 - val_loss: 29.2993\n",
      "Epoch 148/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 27.8820 - val_loss: 29.9056\n",
      "Epoch 149/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 27.9145 - val_loss: 28.9358\n",
      "Epoch 150/1500\n",
      "3600/3600 [==============================] - 1s 195us/step - loss: 27.9850 - val_loss: 28.7518\n",
      "Epoch 151/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 27.8559 - val_loss: 27.6068\n",
      "Epoch 152/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 28.0173 - val_loss: 28.3267\n",
      "Epoch 153/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 27.5213 - val_loss: 29.3555\n",
      "Epoch 154/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 27.9735 - val_loss: 28.6453\n",
      "Epoch 155/1500\n",
      "3600/3600 [==============================] - 1s 210us/step - loss: 27.6862 - val_loss: 27.8929\n",
      "Epoch 156/1500\n",
      "3600/3600 [==============================] - 1s 215us/step - loss: 27.4684 - val_loss: 28.3768\n",
      "Epoch 157/1500\n",
      "3600/3600 [==============================] - 1s 198us/step - loss: 27.6798 - val_loss: 28.5888\n",
      "Epoch 158/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 27.5266 - val_loss: 29.1493\n",
      "Epoch 159/1500\n",
      "3600/3600 [==============================] - 1s 176us/step - loss: 27.9375 - val_loss: 30.3349\n",
      "Epoch 160/1500\n",
      "3600/3600 [==============================] - 1s 177us/step - loss: 28.0702 - val_loss: 28.7591\n",
      "Epoch 161/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 27.8588 - val_loss: 26.4244\n",
      "Epoch 162/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 27.7874 - val_loss: 27.0864\n",
      "Epoch 163/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 27.8681 - val_loss: 28.2703\n",
      "Epoch 164/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 27.7206 - val_loss: 27.8404\n",
      "Epoch 165/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 28.3237 - val_loss: 28.9171\n",
      "Epoch 166/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 27.7535 - val_loss: 28.1138\n",
      "Epoch 167/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 27.3025 - val_loss: 29.3227\n",
      "Epoch 168/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 27.6908 - val_loss: 28.1934\n",
      "Epoch 169/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 27.2053 - val_loss: 27.7074\n",
      "Epoch 170/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 27.3908 - val_loss: 27.2789\n",
      "Epoch 171/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 27.7099 - val_loss: 26.8228\n",
      "Epoch 172/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 27.2421 - val_loss: 29.5786\n",
      "Epoch 173/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 27.5103 - val_loss: 28.3025\n",
      "Epoch 174/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 27.0964 - val_loss: 27.7550\n",
      "Epoch 175/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.8925 - val_loss: 27.4733\n",
      "Epoch 176/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 27.2507 - val_loss: 26.6127\n",
      "Epoch 177/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 27.1441 - val_loss: 27.0551\n",
      "Epoch 178/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 26.7671 - val_loss: 28.6921\n",
      "Epoch 179/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 27.1867 - val_loss: 28.6746\n",
      "Epoch 180/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 27.2259 - val_loss: 25.7962\n",
      "Epoch 181/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 27.5126 - val_loss: 28.8929\n",
      "Epoch 182/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 27.5514 - val_loss: 28.2389\n",
      "Epoch 183/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 27.2540 - val_loss: 28.8177\n",
      "Epoch 184/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 27.0718 - val_loss: 27.9967\n",
      "Epoch 185/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 27.2522 - val_loss: 28.0981\n",
      "Epoch 186/1500\n",
      "3600/3600 [==============================] - 1s 192us/step - loss: 27.0210 - val_loss: 28.4821\n",
      "Epoch 187/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 27.0965 - val_loss: 27.7787\n",
      "Epoch 188/1500\n",
      "3600/3600 [==============================] - 1s 196us/step - loss: 27.1544 - val_loss: 25.6327\n",
      "Epoch 189/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 27.3463 - val_loss: 26.8386\n",
      "Epoch 190/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 26.7811 - val_loss: 26.5325\n",
      "Epoch 191/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 27.1539 - val_loss: 26.1594\n",
      "Epoch 192/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 27.1087 - val_loss: 28.4325\n",
      "Epoch 193/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 27.1226 - val_loss: 27.8185\n",
      "Epoch 194/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 27.0793 - val_loss: 26.0337\n",
      "Epoch 195/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.8100 - val_loss: 26.6702\n",
      "Epoch 196/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.8441 - val_loss: 29.5149\n",
      "Epoch 197/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.9795 - val_loss: 27.7687\n",
      "Epoch 198/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 26.4179 - val_loss: 25.3717\n",
      "Epoch 199/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 26.5211 - val_loss: 29.0080\n",
      "Epoch 200/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 26.6718 - val_loss: 28.2286\n",
      "Epoch 201/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 27.1790 - val_loss: 26.9983\n",
      "Epoch 202/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 26.4816 - val_loss: 26.6823\n",
      "Epoch 203/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.6517 - val_loss: 27.6758\n",
      "Epoch 204/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.5427 - val_loss: 28.0097\n",
      "Epoch 205/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 26.7797 - val_loss: 26.6598\n",
      "Epoch 206/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 26.3148 - val_loss: 29.1837\n",
      "Epoch 207/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 26.3260 - val_loss: 27.7986\n",
      "Epoch 208/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.7012 - val_loss: 26.1103\n",
      "Epoch 209/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 26.5958 - val_loss: 26.9025\n",
      "Epoch 210/1500\n",
      "3600/3600 [==============================] - 1s 190us/step - loss: 26.5376 - val_loss: 27.7615\n",
      "Epoch 211/1500\n",
      "3600/3600 [==============================] - 1s 229us/step - loss: 26.7177 - val_loss: 29.3095\n",
      "Epoch 212/1500\n",
      "3600/3600 [==============================] - 1s 220us/step - loss: 26.4448 - val_loss: 25.5471\n",
      "Epoch 213/1500\n",
      "3600/3600 [==============================] - 1s 181us/step - loss: 26.5960 - val_loss: 28.2059\n",
      "Epoch 214/1500\n",
      "3600/3600 [==============================] - 1s 230us/step - loss: 26.6834 - val_loss: 26.2383\n",
      "Epoch 215/1500\n",
      "3600/3600 [==============================] - 1s 231us/step - loss: 27.0030 - val_loss: 26.1957\n",
      "Epoch 216/1500\n",
      "3600/3600 [==============================] - 1s 216us/step - loss: 26.3995 - val_loss: 29.2981\n",
      "Epoch 217/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 26.6005 - val_loss: 29.4941\n",
      "Epoch 218/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 26.6042 - val_loss: 28.5227\n",
      "Epoch 219/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.8834 - val_loss: 26.0631\n",
      "Epoch 220/1500\n",
      "3600/3600 [==============================] - 1s 209us/step - loss: 26.5822 - val_loss: 29.1826\n",
      "Epoch 221/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.5063 - val_loss: 26.9609\n",
      "Epoch 222/1500\n",
      "3600/3600 [==============================] - 1s 227us/step - loss: 26.5526 - val_loss: 27.7255\n",
      "Epoch 223/1500\n",
      "3600/3600 [==============================] - 1s 225us/step - loss: 27.0064 - val_loss: 26.2795\n",
      "Epoch 224/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600/3600 [==============================] - 1s 160us/step - loss: 26.3488 - val_loss: 29.1540\n",
      "Epoch 225/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.3046 - val_loss: 27.6955\n",
      "Epoch 226/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.0957 - val_loss: 26.9377\n",
      "Epoch 227/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 26.1175 - val_loss: 28.8560\n",
      "Epoch 228/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.0777 - val_loss: 27.2421\n",
      "Epoch 229/1500\n",
      "3600/3600 [==============================] - 1s 203us/step - loss: 26.6076 - val_loss: 26.0542\n",
      "Epoch 230/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 26.0904 - val_loss: 27.3235\n",
      "Epoch 231/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 26.5599 - val_loss: 25.5348\n",
      "Epoch 232/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 26.3111 - val_loss: 25.5650\n",
      "Epoch 233/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 26.3554 - val_loss: 26.6308\n",
      "Epoch 234/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 26.2954 - val_loss: 23.5547\n",
      "Epoch 235/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.9784 - val_loss: 29.2191\n",
      "Epoch 236/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.2259 - val_loss: 27.0340\n",
      "Epoch 237/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 26.7065 - val_loss: 28.5880\n",
      "Epoch 238/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.2533 - val_loss: 27.0567\n",
      "Epoch 239/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.0974 - val_loss: 25.4478\n",
      "Epoch 240/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.8142 - val_loss: 27.0868\n",
      "Epoch 241/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 26.5733 - val_loss: 25.9807\n",
      "Epoch 242/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 26.1081 - val_loss: 26.3515\n",
      "Epoch 243/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.0825 - val_loss: 27.9430\n",
      "Epoch 244/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.9895 - val_loss: 27.3623\n",
      "Epoch 245/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.8732 - val_loss: 28.5803\n",
      "Epoch 246/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.9778 - val_loss: 26.7100\n",
      "Epoch 247/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.9871 - val_loss: 26.7251\n",
      "Epoch 248/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.9896 - val_loss: 26.7304\n",
      "Epoch 249/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.9581 - val_loss: 26.8239\n",
      "Epoch 250/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 26.1020 - val_loss: 26.6919\n",
      "Epoch 251/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 26.5583 - val_loss: 27.7885\n",
      "Epoch 252/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 25.9516 - val_loss: 24.4713\n",
      "Epoch 253/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.6964 - val_loss: 26.2490\n",
      "Epoch 254/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.6069 - val_loss: 25.9068\n",
      "Epoch 255/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.5102 - val_loss: 28.0187\n",
      "Epoch 256/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 26.1958 - val_loss: 27.5751\n",
      "Epoch 257/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.8362 - val_loss: 30.3578\n",
      "Epoch 258/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.6785 - val_loss: 29.1283\n",
      "Epoch 259/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 26.1538 - val_loss: 27.0417\n",
      "Epoch 260/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.8360 - val_loss: 27.3074\n",
      "Epoch 261/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.1450 - val_loss: 25.5182\n",
      "Epoch 262/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 26.1181 - val_loss: 27.4101\n",
      "Epoch 263/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 26.0744 - val_loss: 25.4839\n",
      "Epoch 264/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.6274 - val_loss: 24.8600\n",
      "Epoch 265/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.9367 - val_loss: 25.8406\n",
      "Epoch 266/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 25.6326 - val_loss: 27.8670\n",
      "Epoch 267/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.8109 - val_loss: 26.2211\n",
      "Epoch 268/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 26.0061 - val_loss: 28.6276\n",
      "Epoch 269/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.7192 - val_loss: 26.8365\n",
      "Epoch 270/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.2436 - val_loss: 25.9505\n",
      "Epoch 271/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.5892 - val_loss: 28.9115\n",
      "Epoch 272/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.9541 - val_loss: 27.1367\n",
      "Epoch 273/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.8632 - val_loss: 26.3896\n",
      "Epoch 274/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.8259 - val_loss: 26.2998\n",
      "Epoch 275/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.6153 - val_loss: 25.7376\n",
      "Epoch 276/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.9659 - val_loss: 24.9036\n",
      "Epoch 277/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.8591 - val_loss: 25.6652\n",
      "Epoch 278/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.8477 - val_loss: 26.0557\n",
      "Epoch 279/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 26.4269 - val_loss: 26.1332\n",
      "Epoch 280/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.6481 - val_loss: 25.4760\n",
      "Epoch 281/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.8890 - val_loss: 25.5784\n",
      "Epoch 282/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 26.0355 - val_loss: 27.8477\n",
      "Epoch 283/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5516 - val_loss: 24.0823\n",
      "Epoch 284/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.8158 - val_loss: 26.2382\n",
      "Epoch 285/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 25.4340 - val_loss: 25.1319\n",
      "Epoch 286/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.7076 - val_loss: 24.0964\n",
      "Epoch 287/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.6502 - val_loss: 24.8050\n",
      "Epoch 288/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.7314 - val_loss: 23.6935\n",
      "Epoch 289/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.7782 - val_loss: 29.2391\n",
      "Epoch 290/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.8270 - val_loss: 25.5886\n",
      "Epoch 291/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.5651 - val_loss: 27.6039\n",
      "Epoch 292/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.3002 - val_loss: 26.6306\n",
      "Epoch 293/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.6116 - val_loss: 24.9043\n",
      "Epoch 294/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.6500 - val_loss: 26.3382\n",
      "Epoch 295/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.4538 - val_loss: 23.7557\n",
      "Epoch 296/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5706 - val_loss: 25.1738\n",
      "Epoch 297/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.5074 - val_loss: 27.4776\n",
      "Epoch 298/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.6267 - val_loss: 25.8062\n",
      "Epoch 299/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 25.5036 - val_loss: 23.9890\n",
      "Epoch 300/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.7633 - val_loss: 25.9768\n",
      "Epoch 301/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.6860 - val_loss: 25.7446\n",
      "Epoch 302/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.4524 - val_loss: 24.8257\n",
      "Epoch 303/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 25.7825 - val_loss: 27.0371\n",
      "Epoch 304/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.5777 - val_loss: 27.2811\n",
      "Epoch 305/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5525 - val_loss: 26.4938\n",
      "Epoch 306/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.5646 - val_loss: 26.2119\n",
      "Epoch 307/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.5533 - val_loss: 25.3389\n",
      "Epoch 308/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.3355 - val_loss: 25.5871\n",
      "Epoch 309/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.2272 - val_loss: 24.1146\n",
      "Epoch 310/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.7061 - val_loss: 28.5825\n",
      "Epoch 311/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.3925 - val_loss: 26.8020\n",
      "Epoch 312/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.3429 - val_loss: 25.9962\n",
      "Epoch 313/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 25.8834 - val_loss: 24.9555\n",
      "Epoch 314/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5126 - val_loss: 25.9638\n",
      "Epoch 315/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.3019 - val_loss: 24.7584\n",
      "Epoch 316/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.7971 - val_loss: 26.7246\n",
      "Epoch 317/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.3703 - val_loss: 24.6037\n",
      "Epoch 318/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.2830 - val_loss: 27.3815\n",
      "Epoch 319/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 25.1528 - val_loss: 25.5060\n",
      "Epoch 320/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5668 - val_loss: 25.2183\n",
      "Epoch 321/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.2663 - val_loss: 25.2053\n",
      "Epoch 322/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.1344 - val_loss: 24.9905\n",
      "Epoch 323/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.0577 - val_loss: 25.3388\n",
      "Epoch 324/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5638 - val_loss: 25.8773\n",
      "Epoch 325/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.3238 - val_loss: 25.5343\n",
      "Epoch 326/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 25.2333 - val_loss: 23.5021\n",
      "Epoch 327/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.4447 - val_loss: 23.9388\n",
      "Epoch 328/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.4733 - val_loss: 26.7213\n",
      "Epoch 329/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.4104 - val_loss: 25.3360\n",
      "Epoch 330/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.2498 - val_loss: 25.5546\n",
      "Epoch 331/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 25.2990 - val_loss: 26.8657\n",
      "Epoch 332/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.4455 - val_loss: 26.0215\n",
      "Epoch 333/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.4272 - val_loss: 26.1430\n",
      "Epoch 334/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.3601 - val_loss: 25.1785\n",
      "Epoch 335/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.3174 - val_loss: 25.7781\n",
      "Epoch 336/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.1927 - val_loss: 25.9244\n",
      "Epoch 337/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 25.1538 - val_loss: 25.6083\n",
      "Epoch 338/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.8144 - val_loss: 24.3827\n",
      "Epoch 339/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 25.1935 - val_loss: 26.0307\n",
      "Epoch 340/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.2840 - val_loss: 26.6370\n",
      "Epoch 341/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.5247 - val_loss: 27.9465\n",
      "Epoch 342/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.2588 - val_loss: 26.7799\n",
      "Epoch 343/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.8779 - val_loss: 22.1302\n",
      "Epoch 344/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 25.3161 - val_loss: 25.3159\n",
      "Epoch 345/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.1581 - val_loss: 26.4281\n",
      "Epoch 346/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.3233 - val_loss: 25.0536\n",
      "Epoch 347/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.9731 - val_loss: 24.8874\n",
      "Epoch 348/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.7669 - val_loss: 26.8967\n",
      "Epoch 349/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.1963 - val_loss: 25.6817\n",
      "Epoch 350/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.3421 - val_loss: 25.2943\n",
      "Epoch 351/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.1332 - val_loss: 26.9293\n",
      "Epoch 352/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.2688 - val_loss: 25.9254\n",
      "Epoch 353/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.1872 - val_loss: 26.1003\n",
      "Epoch 354/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 25.7374 - val_loss: 25.5955\n",
      "Epoch 355/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.9320 - val_loss: 26.3354\n",
      "Epoch 356/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 25.2248 - val_loss: 28.1266\n",
      "Epoch 357/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.0338 - val_loss: 26.2716\n",
      "Epoch 358/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 25.0589 - val_loss: 25.5395\n",
      "Epoch 359/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.3054 - val_loss: 27.3225\n",
      "Epoch 360/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.7606 - val_loss: 26.6247\n",
      "Epoch 361/1500\n",
      "3600/3600 [==============================] - 1s 154us/step - loss: 25.4958 - val_loss: 26.9344\n",
      "Epoch 362/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.2544 - val_loss: 24.0660\n",
      "Epoch 363/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.2849 - val_loss: 25.5843\n",
      "Epoch 364/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.3927 - val_loss: 24.7803\n",
      "Epoch 365/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.0180 - val_loss: 27.9485\n",
      "Epoch 366/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.0118 - val_loss: 26.5471\n",
      "Epoch 367/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 25.0750 - val_loss: 25.2041\n",
      "Epoch 368/1500\n",
      "3600/3600 [==============================] - 1s 193us/step - loss: 25.2395 - val_loss: 27.3183\n",
      "Epoch 369/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 24.8927 - val_loss: 25.4664\n",
      "Epoch 370/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 25.1371 - val_loss: 25.1734\n",
      "Epoch 371/1500\n",
      "3600/3600 [==============================] - 1s 226us/step - loss: 25.5187 - val_loss: 23.6111\n",
      "Epoch 372/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600/3600 [==============================] - 1s 208us/step - loss: 25.5705 - val_loss: 25.1294\n",
      "Epoch 373/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 25.2000 - val_loss: 26.8929\n",
      "Epoch 374/1500\n",
      "3600/3600 [==============================] - 1s 272us/step - loss: 25.1186 - val_loss: 25.5760\n",
      "Epoch 375/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.5659 - val_loss: 25.3118\n",
      "Epoch 376/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 26.0961 - val_loss: 27.3337\n",
      "Epoch 377/1500\n",
      "3600/3600 [==============================] - 1s 177us/step - loss: 25.6672 - val_loss: 27.3259\n",
      "Epoch 378/1500\n",
      "3600/3600 [==============================] - 1s 211us/step - loss: 24.6995 - val_loss: 25.0100\n",
      "Epoch 379/1500\n",
      "3600/3600 [==============================] - 1s 242us/step - loss: 25.2165 - val_loss: 24.0702\n",
      "Epoch 380/1500\n",
      "3600/3600 [==============================] - 1s 177us/step - loss: 24.9724 - val_loss: 25.9763\n",
      "Epoch 381/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.8774 - val_loss: 24.8574\n",
      "Epoch 382/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 25.0482 - val_loss: 26.5870\n",
      "Epoch 383/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.9082 - val_loss: 26.1942\n",
      "Epoch 384/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.9210 - val_loss: 24.7179\n",
      "Epoch 385/1500\n",
      "3600/3600 [==============================] - 1s 219us/step - loss: 24.8260 - val_loss: 25.2172\n",
      "Epoch 386/1500\n",
      "3600/3600 [==============================] - 1s 275us/step - loss: 24.9817 - val_loss: 24.6619\n",
      "Epoch 387/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.9307 - val_loss: 24.8157\n",
      "Epoch 388/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.9970 - val_loss: 24.2221\n",
      "Epoch 389/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 24.7011 - val_loss: 25.8789\n",
      "Epoch 390/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.8539 - val_loss: 24.1894\n",
      "Epoch 391/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.7793 - val_loss: 27.7766\n",
      "Epoch 392/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8739 - val_loss: 25.5455\n",
      "Epoch 393/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.9091 - val_loss: 24.9021\n",
      "Epoch 394/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.9132 - val_loss: 25.5704\n",
      "Epoch 395/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8106 - val_loss: 24.3204\n",
      "Epoch 396/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.9795 - val_loss: 25.9769\n",
      "Epoch 397/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 24.7193 - val_loss: 26.1911\n",
      "Epoch 398/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.7251 - val_loss: 25.2470\n",
      "Epoch 399/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.0705 - val_loss: 27.0549\n",
      "Epoch 400/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.7884 - val_loss: 23.8840\n",
      "Epoch 401/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.6695 - val_loss: 25.8788\n",
      "Epoch 402/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.4101 - val_loss: 24.4782\n",
      "Epoch 403/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.3122 - val_loss: 25.1968\n",
      "Epoch 404/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.7055 - val_loss: 26.3808\n",
      "Epoch 405/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.3496 - val_loss: 26.0244\n",
      "Epoch 406/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.5554 - val_loss: 24.6326\n",
      "Epoch 407/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.1958 - val_loss: 26.3242\n",
      "Epoch 408/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.8595 - val_loss: 24.1049\n",
      "Epoch 409/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 25.0883 - val_loss: 26.4963\n",
      "Epoch 410/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 25.3493 - val_loss: 25.3131\n",
      "Epoch 411/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.3451 - val_loss: 26.1216\n",
      "Epoch 412/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.2328 - val_loss: 26.0894\n",
      "Epoch 413/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.0399 - val_loss: 24.6160\n",
      "Epoch 414/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.0230 - val_loss: 25.8175\n",
      "Epoch 415/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 24.6671 - val_loss: 25.5896\n",
      "Epoch 416/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 25.1533 - val_loss: 24.8641\n",
      "Epoch 417/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.9453 - val_loss: 22.9713\n",
      "Epoch 418/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.9668 - val_loss: 23.9254\n",
      "Epoch 419/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.5799 - val_loss: 25.1402\n",
      "Epoch 420/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.3117 - val_loss: 25.2540\n",
      "Epoch 421/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8537 - val_loss: 25.5597\n",
      "Epoch 422/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.9736 - val_loss: 22.9205\n",
      "Epoch 423/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.8570 - val_loss: 25.9960\n",
      "Epoch 424/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.9713 - val_loss: 26.6039\n",
      "Epoch 425/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.6921 - val_loss: 25.8921\n",
      "Epoch 426/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.8568 - val_loss: 23.7201\n",
      "Epoch 427/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.0270 - val_loss: 26.9109\n",
      "Epoch 428/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.7585 - val_loss: 24.1339\n",
      "Epoch 429/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8205 - val_loss: 24.3303\n",
      "Epoch 430/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8292 - val_loss: 25.9287\n",
      "Epoch 431/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 25.4687 - val_loss: 26.4482\n",
      "Epoch 432/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8519 - val_loss: 26.4491\n",
      "Epoch 433/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.5766 - val_loss: 26.2435\n",
      "Epoch 434/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.8074 - val_loss: 25.1735\n",
      "Epoch 435/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.8195 - val_loss: 24.7937\n",
      "Epoch 436/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.6591 - val_loss: 22.8944\n",
      "Epoch 437/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8368 - val_loss: 25.1372\n",
      "Epoch 438/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 25.3762 - val_loss: 25.5310\n",
      "Epoch 439/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.6167 - val_loss: 24.2922\n",
      "Epoch 440/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.6413 - val_loss: 26.1510\n",
      "Epoch 441/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.3818 - val_loss: 25.6476\n",
      "Epoch 442/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 24.8345 - val_loss: 23.7352\n",
      "Epoch 443/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.4478 - val_loss: 25.3149\n",
      "Epoch 444/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.2977 - val_loss: 26.0872\n",
      "Epoch 445/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.7816 - val_loss: 24.1492\n",
      "Epoch 446/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.7504 - val_loss: 23.4344\n",
      "Epoch 447/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.8598 - val_loss: 24.5345\n",
      "Epoch 448/1500\n",
      "3600/3600 [==============================] - 1s 174us/step - loss: 24.4648 - val_loss: 25.7472\n",
      "Epoch 449/1500\n",
      "3600/3600 [==============================] - 1s 176us/step - loss: 24.6868 - val_loss: 24.5550\n",
      "Epoch 450/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.6169 - val_loss: 26.1749\n",
      "Epoch 451/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.5699 - val_loss: 23.8545\n",
      "Epoch 452/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8937 - val_loss: 24.5048\n",
      "Epoch 453/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.3883 - val_loss: 24.0336\n",
      "Epoch 454/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.6697 - val_loss: 26.4665\n",
      "Epoch 455/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.8126 - val_loss: 25.9331\n",
      "Epoch 456/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.4618 - val_loss: 27.7272\n",
      "Epoch 457/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.5122 - val_loss: 25.3461\n",
      "Epoch 458/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.7570 - val_loss: 23.9691\n",
      "Epoch 459/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.6917 - val_loss: 23.0496\n",
      "Epoch 460/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8157 - val_loss: 24.5928\n",
      "Epoch 461/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.9646 - val_loss: 23.9632\n",
      "Epoch 462/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.5682 - val_loss: 25.7063\n",
      "Epoch 463/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.5272 - val_loss: 24.5660\n",
      "Epoch 464/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.7249 - val_loss: 26.7370\n",
      "Epoch 465/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.8931 - val_loss: 26.1337\n",
      "Epoch 466/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 25.3686 - val_loss: 26.3414\n",
      "Epoch 467/1500\n",
      "3600/3600 [==============================] - ETA: 0s - loss: 24.91 - 1s 161us/step - loss: 24.9074 - val_loss: 24.1594\n",
      "Epoch 468/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.7302 - val_loss: 24.5209\n",
      "Epoch 469/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 24.8712 - val_loss: 24.8206\n",
      "Epoch 470/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.5998 - val_loss: 24.8811\n",
      "Epoch 471/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 24.5618 - val_loss: 23.8051\n",
      "Epoch 472/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.4486 - val_loss: 24.9212\n",
      "Epoch 473/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.2527 - val_loss: 24.6496\n",
      "Epoch 474/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.5546 - val_loss: 26.2995\n",
      "Epoch 475/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.5626 - val_loss: 25.0007\n",
      "Epoch 476/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.7226 - val_loss: 26.5784\n",
      "Epoch 477/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.8166 - val_loss: 24.8524\n",
      "Epoch 478/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.9635 - val_loss: 27.4075\n",
      "Epoch 479/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 25.0897 - val_loss: 24.0369\n",
      "Epoch 480/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.4935 - val_loss: 23.6650\n",
      "Epoch 481/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.7270 - val_loss: 26.7965\n",
      "Epoch 482/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.6436 - val_loss: 24.5013\n",
      "Epoch 483/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.6067 - val_loss: 23.6781\n",
      "Epoch 484/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.8590 - val_loss: 25.0915\n",
      "Epoch 485/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.3919 - val_loss: 25.0630\n",
      "Epoch 486/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.3002 - val_loss: 26.4765\n",
      "Epoch 487/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.7976 - val_loss: 24.7218\n",
      "Epoch 488/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.6710 - val_loss: 23.6291\n",
      "Epoch 489/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.7530 - val_loss: 26.7040\n",
      "Epoch 490/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.5892 - val_loss: 26.6674\n",
      "Epoch 491/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 25.0203 - val_loss: 23.2167\n",
      "Epoch 492/1500\n",
      "3600/3600 [==============================] - 1s 159us/step - loss: 25.3941 - val_loss: 23.9414\n",
      "Epoch 493/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.5603 - val_loss: 24.1647\n",
      "Epoch 494/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.6657 - val_loss: 23.6841\n",
      "Epoch 495/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.7549 - val_loss: 23.8297\n",
      "Epoch 496/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.6683 - val_loss: 24.5272\n",
      "Epoch 497/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.5172 - val_loss: 25.3521\n",
      "Epoch 498/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.7064 - val_loss: 26.9003\n",
      "Epoch 499/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.5131 - val_loss: 25.8764\n",
      "Epoch 500/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.4742 - val_loss: 25.0919\n",
      "Epoch 501/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.9264 - val_loss: 25.3514\n",
      "Epoch 502/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 25.3245 - val_loss: 20.2589\n",
      "Epoch 503/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.6687 - val_loss: 22.1325\n",
      "Epoch 504/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.3928 - val_loss: 25.1313\n",
      "Epoch 505/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.6144 - val_loss: 25.0735\n",
      "Epoch 506/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.9247 - val_loss: 22.9332\n",
      "Epoch 507/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.4605 - val_loss: 24.9049\n",
      "Epoch 508/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.3594 - val_loss: 24.3585\n",
      "Epoch 509/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.2877 - val_loss: 24.8109\n",
      "Epoch 510/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.8982 - val_loss: 23.8153\n",
      "Epoch 511/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.1729 - val_loss: 23.5714\n",
      "Epoch 512/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.4115 - val_loss: 24.8032\n",
      "Epoch 513/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 25.0491 - val_loss: 24.9018\n",
      "Epoch 514/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.6622 - val_loss: 24.4011\n",
      "Epoch 515/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.9474 - val_loss: 24.6585\n",
      "Epoch 516/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.1612 - val_loss: 25.9697\n",
      "Epoch 517/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.7166 - val_loss: 26.0126\n",
      "Epoch 518/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 25.0027 - val_loss: 23.4271\n",
      "Epoch 519/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.3908 - val_loss: 25.3552\n",
      "Epoch 520/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.6455 - val_loss: 24.0518\n",
      "Epoch 521/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.4594 - val_loss: 25.7915\n",
      "Epoch 522/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.0188 - val_loss: 24.1061\n",
      "Epoch 523/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.4254 - val_loss: 24.0266\n",
      "Epoch 524/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.8610 - val_loss: 25.0825\n",
      "Epoch 525/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.6008 - val_loss: 24.8741\n",
      "Epoch 526/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 24.5391 - val_loss: 24.8732\n",
      "Epoch 527/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.4020 - val_loss: 24.8397\n",
      "Epoch 528/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.9088 - val_loss: 26.2731\n",
      "Epoch 529/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8699 - val_loss: 25.8249\n",
      "Epoch 530/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 24.7746 - val_loss: 23.9960\n",
      "Epoch 531/1500\n",
      "3600/3600 [==============================] - 1s 173us/step - loss: 24.8830 - val_loss: 24.6674\n",
      "Epoch 532/1500\n",
      "3600/3600 [==============================] - 1s 154us/step - loss: 24.4139 - val_loss: 25.3246\n",
      "Epoch 533/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.4732 - val_loss: 26.2728\n",
      "Epoch 534/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.6300 - val_loss: 24.2983\n",
      "Epoch 535/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.5850 - val_loss: 24.8021\n",
      "Epoch 536/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.6509 - val_loss: 24.1791\n",
      "Epoch 537/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.9714 - val_loss: 24.8278\n",
      "Epoch 538/1500\n",
      "3600/3600 [==============================] - 1s 157us/step - loss: 24.2547 - val_loss: 23.1159\n",
      "Epoch 539/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.2716 - val_loss: 24.8462\n",
      "Epoch 540/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.1778 - val_loss: 25.4337\n",
      "Epoch 541/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.3148 - val_loss: 25.5262\n",
      "Epoch 542/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.8609 - val_loss: 25.0788\n",
      "Epoch 543/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.8379 - val_loss: 24.0805\n",
      "Epoch 544/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.3232 - val_loss: 25.2003\n",
      "Epoch 545/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8333 - val_loss: 25.7834\n",
      "Epoch 546/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 25.0850 - val_loss: 25.9652\n",
      "Epoch 547/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.9095 - val_loss: 25.6751\n",
      "Epoch 548/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.9633 - val_loss: 23.7619\n",
      "Epoch 549/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.6235 - val_loss: 23.8094\n",
      "Epoch 550/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.6542 - val_loss: 22.8675\n",
      "Epoch 551/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.8791 - val_loss: 24.3592\n",
      "Epoch 552/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 24.3035 - val_loss: 24.5732\n",
      "Epoch 553/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.4094 - val_loss: 23.5952\n",
      "Epoch 554/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.9620 - val_loss: 24.6437\n",
      "Epoch 555/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.2212 - val_loss: 24.9645\n",
      "Epoch 556/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.0873 - val_loss: 24.2317\n",
      "Epoch 557/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.8111 - val_loss: 25.1666\n",
      "Epoch 558/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.7855 - val_loss: 24.3894\n",
      "Epoch 559/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.2417 - val_loss: 25.8266\n",
      "Epoch 560/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.4244 - val_loss: 25.0481\n",
      "Epoch 561/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 25.0378 - val_loss: 23.7553\n",
      "Epoch 562/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.4698 - val_loss: 24.6111\n",
      "Epoch 563/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.4958 - val_loss: 25.7475\n",
      "Epoch 564/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.4243 - val_loss: 24.7214\n",
      "Epoch 565/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.7712 - val_loss: 24.9327\n",
      "Epoch 566/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.2588 - val_loss: 23.2313\n",
      "Epoch 567/1500\n",
      "3600/3600 [==============================] - 1s 163us/step - loss: 24.2856 - val_loss: 24.5818\n",
      "Epoch 568/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.4931 - val_loss: 25.9007\n",
      "Epoch 569/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 26.4800 - val_loss: 25.5291\n",
      "Epoch 570/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.6586 - val_loss: 22.9935\n",
      "Epoch 571/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.2294 - val_loss: 24.3963\n",
      "Epoch 572/1500\n",
      "3600/3600 [==============================] - 1s 160us/step - loss: 24.3021 - val_loss: 24.5709\n",
      "Epoch 573/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 25.0475 - val_loss: 24.0680\n",
      "Epoch 574/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.6901 - val_loss: 24.2988\n",
      "Epoch 575/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.2868 - val_loss: 23.6993\n",
      "Epoch 576/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.5836 - val_loss: 25.3603\n",
      "Epoch 577/1500\n",
      "3600/3600 [==============================] - 1s 164us/step - loss: 24.9292 - val_loss: 22.1430\n",
      "Epoch 578/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.4273 - val_loss: 25.7540\n",
      "Epoch 579/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 25.4431 - val_loss: 24.3525\n",
      "Epoch 580/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.4189 - val_loss: 25.2783\n",
      "Epoch 581/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.4422 - val_loss: 24.1651\n",
      "Epoch 582/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.5594 - val_loss: 25.3441\n",
      "Epoch 583/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.3889 - val_loss: 23.1133\n",
      "Epoch 584/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.5700 - val_loss: 24.3888\n",
      "Epoch 585/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.3514 - val_loss: 24.4038\n",
      "Epoch 586/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.1843 - val_loss: 23.0351\n",
      "Epoch 587/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.6894 - val_loss: 24.5116\n",
      "Epoch 588/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 25.4151 - val_loss: 24.0855\n",
      "Epoch 589/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.8585 - val_loss: 25.8596\n",
      "Epoch 590/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.1108 - val_loss: 24.5502\n",
      "Epoch 591/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.2600 - val_loss: 24.5788\n",
      "Epoch 592/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.0470 - val_loss: 25.8503\n",
      "Epoch 593/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.6596 - val_loss: 24.3294\n",
      "Epoch 594/1500\n",
      "3600/3600 [==============================] - 1s 170us/step - loss: 24.0206 - val_loss: 26.3120\n",
      "Epoch 595/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.1503 - val_loss: 26.0232\n",
      "Epoch 596/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.0259 - val_loss: 23.4381\n",
      "Epoch 597/1500\n",
      "3600/3600 [==============================] - 1s 172us/step - loss: 24.6951 - val_loss: 24.4183\n",
      "Epoch 598/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.3754 - val_loss: 24.5737\n",
      "Epoch 599/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.7061 - val_loss: 24.6292\n",
      "Epoch 600/1500\n",
      "3600/3600 [==============================] - 1s 171us/step - loss: 24.4695 - val_loss: 25.1624\n",
      "Epoch 601/1500\n",
      "3600/3600 [==============================] - 1s 168us/step - loss: 24.2637 - val_loss: 22.9425\n",
      "Epoch 602/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.3385 - val_loss: 24.3330\n",
      "Epoch 603/1500\n",
      "3600/3600 [==============================] - 1s 165us/step - loss: 24.1612 - val_loss: 25.2808\n",
      "Epoch 604/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.3455 - val_loss: 25.0124\n",
      "Epoch 605/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.5718 - val_loss: 22.6185\n",
      "Epoch 606/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.2348 - val_loss: 23.2473\n",
      "Epoch 607/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.8566 - val_loss: 26.7794\n",
      "Epoch 608/1500\n",
      "3600/3600 [==============================] - 1s 175us/step - loss: 24.2586 - val_loss: 24.9865\n",
      "Epoch 609/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.4444 - val_loss: 24.1075\n",
      "Epoch 610/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.5437 - val_loss: 24.0537\n",
      "Epoch 611/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.6265 - val_loss: 23.8636\n",
      "Epoch 612/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 25.3957 - val_loss: 25.7684\n",
      "Epoch 613/1500\n",
      "3600/3600 [==============================] - 1s 166us/step - loss: 24.4478 - val_loss: 25.3433\n",
      "Epoch 614/1500\n",
      "3600/3600 [==============================] - 1s 162us/step - loss: 24.1398 - val_loss: 27.1766\n",
      "Epoch 615/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 24.3514 - val_loss: 26.1211\n",
      "Epoch 616/1500\n",
      "3600/3600 [==============================] - 1s 158us/step - loss: 24.1394 - val_loss: 25.4961\n",
      "Epoch 617/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.5510 - val_loss: 24.8504\n",
      "Epoch 618/1500\n",
      "3600/3600 [==============================] - 1s 169us/step - loss: 24.2670 - val_loss: 25.5128\n",
      "Epoch 619/1500\n",
      "3600/3600 [==============================] - 1s 161us/step - loss: 24.3307 - val_loss: 22.3480\n",
      "Epoch 620/1500\n",
      "3600/3600 [==============================] - 1s 167us/step - loss: 24.3788 - val_loss: 24.7597\n",
      "Epoch 621/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 24.2053 - val_loss: 23.4315\n",
      "Epoch 622/1500\n",
      "3600/3600 [==============================] - 1s 192us/step - loss: 24.3891 - val_loss: 25.1994\n",
      "Epoch 623/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 24.2547 - val_loss: 24.0423\n",
      "Epoch 624/1500\n",
      "3600/3600 [==============================] - 1s 180us/step - loss: 24.3026 - val_loss: 23.8117\n",
      "Epoch 625/1500\n",
      "3600/3600 [==============================] - 1s 178us/step - loss: 24.4486 - val_loss: 23.7931\n",
      "Epoch 626/1500\n",
      "3600/3600 [==============================] - 1s 183us/step - loss: 24.7144 - val_loss: 25.1891\n",
      "Epoch 627/1500\n",
      "3600/3600 [==============================] - 1s 209us/step - loss: 24.2326 - val_loss: 24.6062\n",
      "Epoch 628/1500\n",
      "3600/3600 [==============================] - 1s 204us/step - loss: 24.5987 - val_loss: 25.8492\n",
      "Epoch 629/1500\n",
      "3600/3600 [==============================] - 1s 220us/step - loss: 24.5563 - val_loss: 25.6241\n",
      "Epoch 630/1500\n",
      "3600/3600 [==============================] - 1s 213us/step - loss: 24.6835 - val_loss: 24.8213\n",
      "Epoch 631/1500\n",
      "3600/3600 [==============================] - 1s 217us/step - loss: 24.1745 - val_loss: 22.9309\n",
      "Epoch 632/1500\n",
      "3600/3600 [==============================] - 1s 213us/step - loss: 24.0632 - val_loss: 24.2480\n",
      "Epoch 633/1500\n",
      "3600/3600 [==============================] - 1s 209us/step - loss: 24.6408 - val_loss: 24.7343\n",
      "Epoch 634/1500\n",
      "3600/3600 [==============================] - 1s 176us/step - loss: 24.1718 - val_loss: 24.9168\n",
      "Epoch 635/1500\n",
      "3600/3600 [==============================] - 1s 188us/step - loss: 24.2357 - val_loss: 24.2811\n",
      "Epoch 636/1500\n",
      "3600/3600 [==============================] - 1s 205us/step - loss: 24.5512 - val_loss: 25.7479\n",
      "Epoch 637/1500\n",
      "3600/3600 [==============================] - 1s 182us/step - loss: 24.1938 - val_loss: 25.2204\n",
      "Epoch 638/1500\n",
      "3600/3600 [==============================] - 1s 191us/step - loss: 24.2879 - val_loss: 24.8108\n",
      "Epoch 639/1500\n",
      "1504/3600 [===========>..................] - ETA: 0s - loss: 25.0414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-411-eb464a2fbda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2357\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(combined[:4000], seq1[:4000], epochs=1500, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18471844a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start =4814\n",
    "ax = pd.DataFrame(seq1[start:start+1]).squeeze().plot(label='GT')\n",
    "pd.DataFrame(model.predict(combined[start:start+1])).squeeze().plot(ax=ax,label='Pred')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Home number {}\".format(start))\n",
    "plt.savefig(\"/Users/nipun/Desktop/{}.png\".format(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2     1058.0\n",
       "3     1058.0\n",
       "4     1058.0\n",
       "5     1058.0\n",
       "6     1058.0\n",
       "7     1058.0\n",
       "8     1058.0\n",
       "9     1058.0\n",
       "10       0.0\n",
       "11       0.0\n",
       "12       0.0\n",
       "13       0.0\n",
       "14       0.0\n",
       "15       0.0\n",
       "16       0.0\n",
       "17       0.0\n",
       "18       0.0\n",
       "19       0.0\n",
       "20       0.0\n",
       "21       0.0\n",
       "22       0.0\n",
       "23       0.0\n",
       "24       0.0\n",
       "25       0.0\n",
       "26       0.0\n",
       "27       0.0\n",
       "28       0.0\n",
       "29       0.0\n",
       "30       0.0\n",
       "31       0.0\n",
       "32       0.0\n",
       "33       0.0\n",
       "34       0.0\n",
       "35       0.0\n",
       "36       0.0\n",
       "37       0.0\n",
       "38       0.0\n",
       "39       0.0\n",
       "40       0.0\n",
       "41       0.0\n",
       "42       0.0\n",
       "43       0.0\n",
       "44       0.0\n",
       "45       0.0\n",
       "46       0.0\n",
       "47       0.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(seq2[start:start+1]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
